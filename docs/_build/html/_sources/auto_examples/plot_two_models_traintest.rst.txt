
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_two_models_traintest.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_two_models_traintest.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_two_models_traintest.py:


Regression: comparing two tabular models trained on simulated data
====================================================================

This script shows how to train two fusion models on a regression task with train/test protocol and multimodal tabular data.

Key Features:

- Importing models based on name.
- Training and testing models with train/test protocol.
- Saving trained models to a dictionary for later analysis.
- Plotting the results of a single model.
- Plotting the results of multiple models as a bar chart.
- Saving the results of multiple models as a csv file.

.. GENERATED FROM PYTHON SOURCE LINES 16-30

.. code-block:: default


    import importlib

    import matplotlib.pyplot as plt
    from tqdm.auto import tqdm

    from docs.examples import generate_sklearn_simulated_data
    from fusionlibrary.data import get_data_module
    from fusionlibrary.eval import Plotter
    from fusionlibrary.fusion_models.base_model import BaseModel
    from fusionlibrary.train import train_and_save_models
    from fusionlibrary.utils.model_chooser import get_models









.. GENERATED FROM PYTHON SOURCE LINES 31-42

1. Import fusion models
------------------------
Here we import the fusion models to be compared. The models are imported using the
:func:`~fusionlibrary.utils.model_chooser.get_models` function, which takes a dictionary of conditions
as an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.

The function returns a dataframe of the models that match the conditions. The dataframe contains the
method name, the class name, the modality type, the fusion type, the path to the model, and the path to the
model's parent class. The paths are used to import the models with the :func:`importlib.import_module`.

We're importing ConcatTabularData and TabularChannelWiseMultiAttention models for this example. Both are multimodal tabular models.

.. GENERATED FROM PYTHON SOURCE LINES 42-59

.. code-block:: default


    model_conditions = {
        "class_name": ["ConcatTabularData", "TabularChannelWiseMultiAttention"],
    }

    imported_models = get_models(model_conditions)
    print("Imported methods:")
    print(imported_models.method_name.values)

    fusion_models = []  # contains the class objects for each model
    for index, row in imported_models.iterrows():
        module = importlib.import_module(row["method_path"])
        module_class = getattr(module, row["class_name"])

        fusion_models.append(module_class)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Imported methods:
    ['Concatenating tabular data' 'Channel-wise multiplication net (tabular)']




.. GENERATED FROM PYTHON SOURCE LINES 60-72

2. Set the training parameters
--------------------------------
Here we define the parameters for training and testing the models. The parameters are stored in a dictionary and passed to most
of the methods in this library.
For training and testing, the necessary parameters are:

- ``test_size``: the proportion of the data to be used for testing.
- ``kfold_flag``: the user sets this to False for train/test protocol.
- ``log``: a boolean of whether to log the results using Weights and Biases.
- ``pred_type``: the type of prediction to be performed. This is either ``regression``, ``binary``, or ``classification``. For this example we're using regression.

If we were going to use a subspace-based fusion model, we would also need to set the latent dimensionality of the subspace with ``subspace_latdims``. This will be shown in a different example.

.. GENERATED FROM PYTHON SOURCE LINES 72-81

.. code-block:: default


    params = {
        "test_size": 0.2,
        "kfold_flag": False,
        "log": False,
        "pred_type": "regression",
    }









.. GENERATED FROM PYTHON SOURCE LINES 82-86

3. Generating simulated data
--------------------------------
Here we generate simulated data for the two tabular modalities for this example.
This function also simulated image data which we aren't using here.

.. GENERATED FROM PYTHON SOURCE LINES 86-95

.. code-block:: default


    params = generate_sklearn_simulated_data(
        num_samples=500,
        num_tab1_features=10,
        num_tab2_features=10,
        img_dims=(1, 100, 100),
        params=params,
    )








.. GENERATED FROM PYTHON SOURCE LINES 96-108

4. Training the first fusion model
----------------------------------
Here we train the first fusion model. We're using the ``train_and_save_models`` function to train and test the models.
This function takes the following inputs:

- ``trained_models_dict``: a dictionary to store the trained models.
- ``data_module``: the data module containing the data.
- ``params``: the parameters for training and testing.
- ``fusion_model``: the fusion model to be trained.
- ``init_model``: the initialised dummy fusion model.

First we'll create a dictionary to store both the trained models so we can compare them later.

.. GENERATED FROM PYTHON SOURCE LINES 108-110

.. code-block:: default

    all_trained_models = {}  # create dictionary to store trained models








.. GENERATED FROM PYTHON SOURCE LINES 111-120

To train the first model we need to:

1. *Choose the model*: We're using the first model in the ``fusion_models`` list we made earlier.
2. *Create a dictionary to store the trained model*: We're using the name of the model as the key. It may seem overkill to make a dictionary just to store one model, but we also use this when we do k-fold training to store the trained models from the different folds.
3. *Initialise the model with dummy data*: This is so we can find out whether there are extra instructions for creating the datamodule (such as a method for creating a graph datamodule).
4. *Print the attributes of the model*: To check it's been initialised correctly.
5. *Create the datamodule*: This is done with the :func:`~fusionlibrary.data.get_data_module` function. This function takes the initialised model and the parameters as inputs. It returns the datamodule.
6. *Train and test the model*: This is done with the :func:`~fusionlibrary.train.train_and_save_models` function. This function takes the trained_models_dict, the datamodule, the parameters, the fusion model, and the initialised model as inputs. It returns the trained_models_dict with the trained model added to it.
7. *Add the trained model to the ``all_trained_models`` dictionary*: This is so we can compare the results of the two models later.

.. GENERATED FROM PYTHON SOURCE LINES 120-143

.. code-block:: default


    fusion_model = fusion_models[0]
    single_model_dict = {}

    print("Method name:", fusion_model.method_name)
    print("Modality type:", fusion_model.modality_type)
    print("Fusion type:", fusion_model.fusion_type)

    # Create the data module
    dm = get_data_module(fusion_model=fusion_model, params=params)

    # Train and test
    single_model_dict = train_and_save_models(
        trained_models_dict=single_model_dict,
        data_module=dm,
        params=params,
        fusion_model=fusion_model,
    )

    # Add trained model to dictionary
    all_trained_models[fusion_model.__name__] = single_model_dict[fusion_model.__name__]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Method name: Concatenating tabular data
    Modality type: both_tab
    Fusion type: operation

    Training: 0it [00:00, ?it/s]
    Training:   0%|          | 0/63 [00:00<?, ?it/s]
    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s] 
    Epoch 0:   2%|▏         | 1/63 [00:00<00:00, 65.22it/s]
    Epoch 0:   2%|▏         | 1/63 [00:00<00:00, 64.40it/s, loss=30.7]
    Epoch 0:   3%|▎         | 2/63 [00:00<00:00, 101.40it/s, loss=30.7]
    Epoch 0:   3%|▎         | 2/63 [00:00<00:00, 100.37it/s, loss=38.9]
    Epoch 0:   5%|▍         | 3/63 [00:00<00:00, 135.51it/s, loss=38.9]
    Epoch 0:   5%|▍         | 3/63 [00:00<00:00, 134.69it/s, loss=33.3]
    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 166.20it/s, loss=33.3]
    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 164.99it/s, loss=30.9]
    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 190.64it/s, loss=30.9]
    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 189.49it/s, loss=30.8]
    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 213.03it/s, loss=30.8]
    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 212.14it/s, loss=31.6]
    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 234.26it/s, loss=31.6]
    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 233.37it/s, loss=30.9]
    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 252.85it/s, loss=30.9]
    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 251.60it/s, loss=34.3]
    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 267.37it/s, loss=34.3]
    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 266.04it/s, loss=34.9]
    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 279.42it/s, loss=34.9]
    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 278.30it/s, loss=36.3]
    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 292.14it/s, loss=36.3]
    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 291.04it/s, loss=35.5]
    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 304.03it/s, loss=35.5]
    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 303.10it/s, loss=34.7]
    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 315.04it/s, loss=34.7]
    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 313.96it/s, loss=34.5]
    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 324.46it/s, loss=34.5]
    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 323.51it/s, loss=35.3]
    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 332.92it/s, loss=35.3]
    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 331.90it/s, loss=35.7]
    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 340.48it/s, loss=35.7]
    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 339.33it/s, loss=36.7]
    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 345.80it/s, loss=36.7]
    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 344.65it/s, loss=35.6]
    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 351.83it/s, loss=35.6]
    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 350.87it/s, loss=35.9]
    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 357.86it/s, loss=35.9]
    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 356.74it/s, loss=38]  
    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 362.77it/s, loss=38]
    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 361.77it/s, loss=37.5]
    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 366.30it/s, loss=37.5]
    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 365.50it/s, loss=38.7]
    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 371.04it/s, loss=38.7]
    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 370.08it/s, loss=38.4]
    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 376.09it/s, loss=38.4]
    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 375.36it/s, loss=38.6]
    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 381.04it/s, loss=38.6]
    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 380.14it/s, loss=37.9]
    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 385.05it/s, loss=37.9]
    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 384.25it/s, loss=36.8]
    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 388.83it/s, loss=36.8]
    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 387.93it/s, loss=35.7]
    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 391.61it/s, loss=35.7]
    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 390.83it/s, loss=35.7]
    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 395.38it/s, loss=35.7]
    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 394.58it/s, loss=34.6]
    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 397.45it/s, loss=34.6]
    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 396.71it/s, loss=34.1]
    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 400.96it/s, loss=34.1]
    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 400.17it/s, loss=33.8]
    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 403.61it/s, loss=33.8]
    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 402.89it/s, loss=33.8]
    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 406.52it/s, loss=33.8]
    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 405.79it/s, loss=34.1]
    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 408.93it/s, loss=34.1]
    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 408.04it/s, loss=33.2]
    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 410.81it/s, loss=33.2]
    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 410.14it/s, loss=32.6]
    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 413.60it/s, loss=32.6]
    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 412.99it/s, loss=31.4]
    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 416.50it/s, loss=31.4]
    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 415.81it/s, loss=29.7]
    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 418.36it/s, loss=29.7]
    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 417.62it/s, loss=33.1]
    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 420.23it/s, loss=33.1]
    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 419.58it/s, loss=32.9]
    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 422.06it/s, loss=32.9]
    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 421.42it/s, loss=29.8]
    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 424.21it/s, loss=29.8]
    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 423.50it/s, loss=30]  
    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 425.95it/s, loss=30]
    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 425.41it/s, loss=29.5]
    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 429.04it/s, loss=29.5]
    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 428.55it/s, loss=29]  
    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 431.90it/s, loss=29]
    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 431.32it/s, loss=29.4]
    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 433.68it/s, loss=29.4]
    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 433.07it/s, loss=31.4]
    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 435.70it/s, loss=31.4]
    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 435.24it/s, loss=33.3]
    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 438.29it/s, loss=33.3]
    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 437.61it/s, loss=33.7]
    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 439.26it/s, loss=33.7]
    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 438.76it/s, loss=35.3]
    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 441.58it/s, loss=35.3]
    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 441.13it/s, loss=35.3]
    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 443.17it/s, loss=35.3]
    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 442.71it/s, loss=35.6]
    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 445.81it/s, loss=35.6]
    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 445.39it/s, loss=35.2]
    Epoch 0:  81%|████████  | 51/63 [00:00<00:00, 449.49it/s, loss=35.2]
    Epoch 0:  83%|████████▎ | 52/63 [00:00<00:00, 456.43it/s, loss=35.2]
    Epoch 0:  84%|████████▍ | 53/63 [00:00<00:00, 463.47it/s, loss=35.2]
    Epoch 0:  86%|████████▌ | 54/63 [00:00<00:00, 470.46it/s, loss=35.2]
    Epoch 0:  87%|████████▋ | 55/63 [00:00<00:00, 477.45it/s, loss=35.2]
    Epoch 0:  89%|████████▉ | 56/63 [00:00<00:00, 484.41it/s, loss=35.2]
    Epoch 0:  90%|█████████ | 57/63 [00:00<00:00, 491.32it/s, loss=35.2]
    Epoch 0:  92%|█████████▏| 58/63 [00:00<00:00, 498.18it/s, loss=35.2]
    Epoch 0:  94%|█████████▎| 59/63 [00:00<00:00, 505.00it/s, loss=35.2]
    Epoch 0:  95%|█████████▌| 60/63 [00:00<00:00, 511.77it/s, loss=35.2]
    Epoch 0:  97%|█████████▋| 61/63 [00:00<00:00, 518.51it/s, loss=35.2]
    Epoch 0:  98%|█████████▊| 62/63 [00:00<00:00, 525.21it/s, loss=35.2]
    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 531.93it/s, loss=35.2]
    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 526.01it/s, loss=35.2, val_loss=30.90]
    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 524.56it/s, loss=35.2, val_loss=30.90, train_loss=34.80]
    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s, loss=35.2, val_loss=30.90, train_loss=34.80]          
    Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=35.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 559.32it/s, loss=35.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 521.94it/s, loss=34.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 575.23it/s, loss=34.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 553.41it/s, loss=34.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 562.97it/s, loss=34.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 548.16it/s, loss=35.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 561.17it/s, loss=35.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 551.97it/s, loss=36.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 571.84it/s, loss=36.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 564.66it/s, loss=36.5, val_loss=30.90, train_loss=34.80]
    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 581.34it/s, loss=36.5, val_loss=30.90, train_loss=34.80]
    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 575.10it/s, loss=36.6, val_loss=30.90, train_loss=34.80]
    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 587.54it/s, loss=36.6, val_loss=30.90, train_loss=34.80]
    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 581.93it/s, loss=34.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 591.41it/s, loss=34.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 586.51it/s, loss=34.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 588.47it/s, loss=34.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 583.70it/s, loss=34.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 589.20it/s, loss=34.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 583.95it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 582.60it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 577.97it/s, loss=35.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 575.94it/s, loss=35.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 572.36it/s, loss=36.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 575.33it/s, loss=36.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 572.41it/s, loss=36.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 577.30it/s, loss=36.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 573.94it/s, loss=36, val_loss=30.90, train_loss=34.80]  
    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 575.88it/s, loss=36, val_loss=30.90, train_loss=34.80]
    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 572.56it/s, loss=35.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 574.24it/s, loss=35.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 571.70it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 576.04it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 573.71it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 576.21it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 573.65it/s, loss=36.7, val_loss=30.90, train_loss=34.80]
    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 575.65it/s, loss=36.7, val_loss=30.90, train_loss=34.80]
    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 572.96it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 573.18it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 570.79it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 569.26it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 566.76it/s, loss=37.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 567.01it/s, loss=37.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 565.31it/s, loss=39.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 569.04it/s, loss=39.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 567.50it/s, loss=37.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 569.53it/s, loss=37.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 567.23it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 566.11it/s, loss=36.4, val_loss=30.90, train_loss=34.80]
    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 564.45it/s, loss=37.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 564.23it/s, loss=37.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 562.55it/s, loss=37.6, val_loss=30.90, train_loss=34.80]
    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 562.72it/s, loss=37.6, val_loss=30.90, train_loss=34.80]
    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 561.11it/s, loss=36.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 561.55it/s, loss=36.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 560.25it/s, loss=37.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 563.10it/s, loss=37.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 561.70it/s, loss=39, val_loss=30.90, train_loss=34.80]  
    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 563.26it/s, loss=39, val_loss=30.90, train_loss=34.80]
    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 561.76it/s, loss=39.7, val_loss=30.90, train_loss=34.80]
    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 561.82it/s, loss=39.7, val_loss=30.90, train_loss=34.80]
    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 560.39it/s, loss=37.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 561.04it/s, loss=37.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 559.90it/s, loss=35.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 562.75it/s, loss=35.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 561.66it/s, loss=36.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 564.48it/s, loss=36.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 563.42it/s, loss=36.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 565.63it/s, loss=36.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 564.30it/s, loss=35.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 565.30it/s, loss=35.9, val_loss=30.90, train_loss=34.80]
    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 564.28it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 565.73it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 564.72it/s, loss=34.6, val_loss=30.90, train_loss=34.80]
    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 566.64it/s, loss=34.6, val_loss=30.90, train_loss=34.80]
    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 565.45it/s, loss=33.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 566.42it/s, loss=33.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 565.45it/s, loss=32.5, val_loss=30.90, train_loss=34.80]
    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 566.45it/s, loss=32.5, val_loss=30.90, train_loss=34.80]
    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 565.52it/s, loss=34.7, val_loss=30.90, train_loss=34.80]
    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 567.56it/s, loss=34.7, val_loss=30.90, train_loss=34.80]
    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 566.66it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 567.81it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 566.79it/s, loss=33, val_loss=30.90, train_loss=34.80]  
    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 568.17it/s, loss=33, val_loss=30.90, train_loss=34.80]
    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 567.31it/s, loss=34.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 569.45it/s, loss=34.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 568.66it/s, loss=34.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 570.37it/s, loss=34.1, val_loss=30.90, train_loss=34.80]
    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 569.11it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 568.84it/s, loss=34.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 568.01it/s, loss=34.5, val_loss=30.90, train_loss=34.80]
    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 569.98it/s, loss=34.5, val_loss=30.90, train_loss=34.80]
    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 569.22it/s, loss=35.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 570.96it/s, loss=35.3, val_loss=30.90, train_loss=34.80]
    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 570.11it/s, loss=33.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 571.32it/s, loss=33.8, val_loss=30.90, train_loss=34.80]
    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 570.40it/s, loss=32.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 569.35it/s, loss=32.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 568.50it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  81%|████████  | 51/63 [00:00<00:00, 570.55it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  83%|████████▎ | 52/63 [00:00<00:00, 578.63it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  84%|████████▍ | 53/63 [00:00<00:00, 586.95it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  86%|████████▌ | 54/63 [00:00<00:00, 595.19it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  87%|████████▋ | 55/63 [00:00<00:00, 603.36it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  89%|████████▉ | 56/63 [00:00<00:00, 611.51it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  90%|█████████ | 57/63 [00:00<00:00, 619.61it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  92%|█████████▏| 58/63 [00:00<00:00, 627.63it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  94%|█████████▎| 59/63 [00:00<00:00, 635.56it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  95%|█████████▌| 60/63 [00:00<00:00, 643.54it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  97%|█████████▋| 61/63 [00:00<00:00, 651.04it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1:  98%|█████████▊| 62/63 [00:00<00:00, 658.87it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 666.75it/s, loss=31.2, val_loss=30.90, train_loss=34.80]
    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 661.04it/s, loss=31.2, val_loss=30.40, train_loss=34.80]
    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 658.83it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=31.2, val_loss=30.40, train_loss=34.50]          
    Epoch 2:   0%|          | 0/63 [00:00<?, ?it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 518.58it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 483.05it/s, loss=31.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 557.53it/s, loss=31.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 539.36it/s, loss=31.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 558.12it/s, loss=31.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 546.63it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 573.80it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 564.15it/s, loss=29.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 576.43it/s, loss=29.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 568.63it/s, loss=30.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 581.21it/s, loss=30.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 574.98it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 588.13it/s, loss=31.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 582.54it/s, loss=32.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 590.10it/s, loss=32.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 584.02it/s, loss=32.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 582.71it/s, loss=32.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 578.51it/s, loss=33.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 582.23it/s, loss=33.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 577.39it/s, loss=29.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 574.28it/s, loss=29.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 570.10it/s, loss=29, val_loss=30.40, train_loss=34.50]  
    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 572.73it/s, loss=29, val_loss=30.40, train_loss=34.50]
    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 569.68it/s, loss=28.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 576.49it/s, loss=28.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 573.64it/s, loss=28.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 577.55it/s, loss=28.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 574.05it/s, loss=28.8, val_loss=30.40, train_loss=34.50]
    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 574.05it/s, loss=28.8, val_loss=30.40, train_loss=34.50]
    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 571.00it/s, loss=27.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 574.54it/s, loss=27.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 572.16it/s, loss=29.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 577.42it/s, loss=29.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 574.69it/s, loss=28.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 575.10it/s, loss=28.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 572.54it/s, loss=29.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 574.48it/s, loss=29.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 572.68it/s, loss=30.8, val_loss=30.40, train_loss=34.50]
    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 578.56it/s, loss=30.8, val_loss=30.40, train_loss=34.50]
    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 576.61it/s, loss=32.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 581.54it/s, loss=32.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 579.82it/s, loss=33.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 584.31it/s, loss=33.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 582.67it/s, loss=33.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 586.27it/s, loss=33.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 584.02it/s, loss=35.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 584.27it/s, loss=35.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 582.61it/s, loss=36, val_loss=30.40, train_loss=34.50]  
    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 582.38it/s, loss=36, val_loss=30.40, train_loss=34.50]
    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 580.85it/s, loss=35.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 583.99it/s, loss=35.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 582.51it/s, loss=34.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 585.39it/s, loss=34.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 583.94it/s, loss=33.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 586.65it/s, loss=33.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 585.36it/s, loss=32.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 588.65it/s, loss=32.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 587.38it/s, loss=32.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 590.26it/s, loss=32.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 588.89it/s, loss=33.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 587.82it/s, loss=33.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 586.15it/s, loss=33.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 586.65it/s, loss=33.2, val_loss=30.40, train_loss=34.50]
    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 585.41it/s, loss=34.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 587.73it/s, loss=34.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 586.60it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 588.19it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 587.11it/s, loss=34.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 589.60it/s, loss=34.9, val_loss=30.40, train_loss=34.50]
    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 588.55it/s, loss=35, val_loss=30.40, train_loss=34.50]  
    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 591.29it/s, loss=35, val_loss=30.40, train_loss=34.50]
    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 590.25it/s, loss=33.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 592.78it/s, loss=33.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 591.76it/s, loss=34.8, val_loss=30.40, train_loss=34.50]
    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 593.58it/s, loss=34.8, val_loss=30.40, train_loss=34.50]
    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 592.04it/s, loss=35.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 591.11it/s, loss=35.1, val_loss=30.40, train_loss=34.50]
    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 590.07it/s, loss=34.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 591.84it/s, loss=34.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 590.91it/s, loss=33.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 590.35it/s, loss=33.3, val_loss=30.40, train_loss=34.50]
    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 589.28it/s, loss=33, val_loss=30.40, train_loss=34.50]  
    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 590.96it/s, loss=33, val_loss=30.40, train_loss=34.50]
    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 590.08it/s, loss=33.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 591.90it/s, loss=33.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 591.02it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 592.18it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 591.32it/s, loss=34.6, val_loss=30.40, train_loss=34.50]
    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 592.60it/s, loss=34.6, val_loss=30.40, train_loss=34.50]
    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 591.50it/s, loss=34.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 591.85it/s, loss=34.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 591.00it/s, loss=34, val_loss=30.40, train_loss=34.50]  
    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 592.75it/s, loss=34, val_loss=30.40, train_loss=34.50]
    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 591.92it/s, loss=35, val_loss=30.40, train_loss=34.50]
    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 592.64it/s, loss=35, val_loss=30.40, train_loss=34.50]
    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 591.64it/s, loss=34.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 591.35it/s, loss=34.4, val_loss=30.40, train_loss=34.50]
    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 590.28it/s, loss=35.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 590.49it/s, loss=35.5, val_loss=30.40, train_loss=34.50]
    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 589.48it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  81%|████████  | 51/63 [00:00<00:00, 590.61it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  83%|████████▎ | 52/63 [00:00<00:00, 598.81it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  84%|████████▍ | 53/63 [00:00<00:00, 607.21it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  86%|████████▌ | 54/63 [00:00<00:00, 615.64it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  87%|████████▋ | 55/63 [00:00<00:00, 623.94it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  89%|████████▉ | 56/63 [00:00<00:00, 632.30it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  90%|█████████ | 57/63 [00:00<00:00, 640.59it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  92%|█████████▏| 58/63 [00:00<00:00, 648.64it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  94%|█████████▎| 59/63 [00:00<00:00, 656.66it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  95%|█████████▌| 60/63 [00:00<00:00, 664.72it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  97%|█████████▋| 61/63 [00:00<00:00, 672.68it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2:  98%|█████████▊| 62/63 [00:00<00:00, 680.62it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 688.55it/s, loss=34.7, val_loss=30.40, train_loss=34.50]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 681.91it/s, loss=34.7, val_loss=28.40, train_loss=34.50]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 679.30it/s, loss=34.7, val_loss=28.40, train_loss=33.10]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 663.45it/s, loss=34.7, val_loss=28.40, train_loss=33.10]
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         Validate metric           DataLoader 0
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
             MAE_val             4.256635665893555
             R2_val             0.08410525321960449
            val_loss             28.38005828857422
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────




.. GENERATED FROM PYTHON SOURCE LINES 144-148

5. Plotting the results of the first model
--------------------------------------------
We're using the :class:`~fusionlibrary.eval.Plotter` class to plot the results of the first model. This class takes the dictionary of trained models and the parameters as inputs. It returns a dictionary of figures.
If there is one model in the dictionary (i.e. only one unique key), then it plots the figures for analysing the results of a single model.

.. GENERATED FROM PYTHON SOURCE LINES 148-153

.. code-block:: default


    plotter = Plotter(single_model_dict, params)
    single_model_figures_dict = plotter.plot_all()
    plotter.show_all(single_model_figures_dict)




.. image-sg:: /auto_examples/images/sphx_glr_plot_two_models_traintest_001.png
   :alt: ConcatTabularData - Validation R2: 0.084
   :srcset: /auto_examples/images/sphx_glr_plot_two_models_traintest_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Plotting models ['ConcatTabularData'] ...
    Plotting results of a single model.
    ConcatTabularData_reals_vs_preds




.. GENERATED FROM PYTHON SOURCE LINES 154-157

6. Training the second fusion model
-------------------------------------
Here we train the second fusion model: TabularChannelWiseMultiAttention. We're using the same steps as before, but this time we're using the second model in the ``fusion_models`` list.

.. GENERATED FROM PYTHON SOURCE LINES 160-161

Choose the model

.. GENERATED FROM PYTHON SOURCE LINES 161-184

.. code-block:: default

    fusion_model = fusion_models[1]
    single_model_dict = {}


    print("Method name:", fusion_model.method_name)
    print("Modality type:", fusion_model.modality_type)
    print("Fusion type:", fusion_model.fusion_type)

    # Create the data module
    dm = get_data_module(fusion_model=fusion_model, params=params)

    # Train and test
    trained_models = train_and_save_models(
        trained_models_dict=single_model_dict,
        data_module=dm,
        params=params,
        fusion_model=fusion_model,
    )

    # Add trained model to dictionary
    all_trained_models[fusion_model.__name__] = single_model_dict[fusion_model.__name__]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Method name: Channel-wise multiplication net (tabular)
    Modality type: both_tab
    Fusion type: attention

    Training: 0it [00:00, ?it/s]
    Training:   0%|          | 0/63 [00:00<?, ?it/s]
    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s] 
    Epoch 0:   2%|▏         | 1/63 [00:00<00:00, 160.96it/s]
    Epoch 0:   2%|▏         | 1/63 [00:00<00:00, 155.60it/s, loss=29]
    Epoch 0:   3%|▎         | 2/63 [00:00<00:00, 182.15it/s, loss=29]
    Epoch 0:   3%|▎         | 2/63 [00:00<00:00, 179.55it/s, loss=34.2]
    Epoch 0:   5%|▍         | 3/63 [00:00<00:00, 197.89it/s, loss=34.2]
    Epoch 0:   5%|▍         | 3/63 [00:00<00:00, 195.93it/s, loss=34.1]
    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 204.44it/s, loss=34.1]
    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 202.49it/s, loss=30.4]
    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 207.31it/s, loss=30.4]
    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 205.58it/s, loss=28.3]
    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 208.57it/s, loss=28.3]
    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 207.23it/s, loss=31.2]
    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 211.65it/s, loss=31.2]
    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 210.63it/s, loss=36.1]
    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 211.86it/s, loss=36.1]
    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 210.71it/s, loss=34]  
    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 211.82it/s, loss=34]
    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 210.87it/s, loss=33.7]
    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 212.97it/s, loss=33.7]
    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 212.28it/s, loss=33.3]
    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 214.10it/s, loss=33.3]
    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 213.45it/s, loss=34]  
    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 214.50it/s, loss=34]
    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 213.70it/s, loss=33.8]
    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 213.97it/s, loss=33.8]
    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 213.32it/s, loss=33.4]
    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 215.17it/s, loss=33.4]
    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 214.67it/s, loss=32.7]
    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 216.10it/s, loss=32.7]
    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 215.61it/s, loss=32.9]
    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 217.44it/s, loss=32.9]
    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 217.00it/s, loss=32.9]
    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 218.61it/s, loss=32.9]
    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 218.07it/s, loss=33.6]
    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 218.80it/s, loss=33.6]
    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 218.34it/s, loss=36]  
    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 218.78it/s, loss=36]
    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 218.30it/s, loss=34.7]
    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 218.13it/s, loss=34.7]
    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 217.83it/s, loss=33.4]
    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 218.27it/s, loss=33.4]
    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 217.81it/s, loss=33.8]
    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 218.09it/s, loss=33.8]
    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 217.73it/s, loss=34.2]
    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 217.74it/s, loss=34.2]
    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 217.32it/s, loss=35]  
    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 217.03it/s, loss=35]
    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 216.69it/s, loss=35.9]
    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 216.94it/s, loss=35.9]
    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 216.66it/s, loss=37.5]
    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 217.30it/s, loss=37.5]
    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 216.96it/s, loss=36.4]
    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 217.16it/s, loss=36.4]
    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 216.80it/s, loss=34]  
    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 217.04it/s, loss=34]
    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 216.76it/s, loss=35.3]
    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 216.75it/s, loss=35.3]
    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 216.51it/s, loss=35.7]
    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 217.60it/s, loss=35.7]
    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 217.29it/s, loss=35.8]
    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 217.46it/s, loss=35.8]
    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 217.25it/s, loss=35.9]
    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 218.24it/s, loss=35.9]
    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 217.96it/s, loss=35.4]
    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 218.97it/s, loss=35.4]
    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 218.77it/s, loss=34.6]
    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 220.28it/s, loss=34.6]
    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 220.05it/s, loss=36.3]
    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 220.88it/s, loss=36.3]
    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 220.62it/s, loss=35.6]
    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 221.32it/s, loss=35.6]
    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 221.13it/s, loss=35.7]
    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 221.93it/s, loss=35.7]
    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 221.75it/s, loss=37.5]
    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 222.91it/s, loss=37.5]
    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 222.72it/s, loss=35.6]
    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 223.85it/s, loss=35.6]
    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 223.67it/s, loss=37.7]
    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 224.43it/s, loss=37.7]
    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 224.25it/s, loss=38.1]
    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 225.25it/s, loss=38.1]
    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 225.08it/s, loss=38.5]
    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 225.41it/s, loss=38.5]
    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 225.23it/s, loss=37.3]
    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 226.11it/s, loss=37.3]
    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 225.95it/s, loss=35.2]
    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 226.83it/s, loss=35.2]
    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 226.63it/s, loss=34.5]
    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 226.89it/s, loss=34.5]
    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 226.71it/s, loss=32.8]
    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 227.02it/s, loss=32.8]
    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 226.86it/s, loss=34.6]
    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 227.66it/s, loss=34.6]
    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 227.51it/s, loss=34.9]
    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 228.02it/s, loss=34.9]
    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 227.84it/s, loss=34.4]
    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 228.12it/s, loss=34.4]
    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 227.97it/s, loss=33.6]
    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 228.46it/s, loss=33.6]
    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 228.34it/s, loss=34]  
    Epoch 0:  81%|████████  | 51/63 [00:00<00:00, 231.30it/s, loss=34]
    Epoch 0:  83%|████████▎ | 52/63 [00:00<00:00, 235.19it/s, loss=34]
    Epoch 0:  84%|████████▍ | 53/63 [00:00<00:00, 239.12it/s, loss=34]
    Epoch 0:  86%|████████▌ | 54/63 [00:00<00:00, 243.04it/s, loss=34]
    Epoch 0:  87%|████████▋ | 55/63 [00:00<00:00, 246.95it/s, loss=34]
    Epoch 0:  89%|████████▉ | 56/63 [00:00<00:00, 250.85it/s, loss=34]
    Epoch 0:  90%|█████████ | 57/63 [00:00<00:00, 254.74it/s, loss=34]
    Epoch 0:  92%|█████████▏| 58/63 [00:00<00:00, 258.60it/s, loss=34]
    Epoch 0:  94%|█████████▎| 59/63 [00:00<00:00, 262.45it/s, loss=34]
    Epoch 0:  95%|█████████▌| 60/63 [00:00<00:00, 266.28it/s, loss=34]
    Epoch 0:  97%|█████████▋| 61/63 [00:00<00:00, 270.10it/s, loss=34]
    Epoch 0:  98%|█████████▊| 62/63 [00:00<00:00, 273.90it/s, loss=34]
    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 277.70it/s, loss=34]
    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 275.99it/s, loss=34, val_loss=32.40]
    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 275.52it/s, loss=34, val_loss=32.40, train_loss=34.60]
    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s, loss=34, val_loss=32.40, train_loss=34.60]          
    Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=34, val_loss=32.40, train_loss=34.60]
    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 228.58it/s, loss=34, val_loss=32.40, train_loss=34.60]
    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 219.98it/s, loss=32.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 241.32it/s, loss=32.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 236.91it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 247.81it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 244.90it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 253.31it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 250.93it/s, loss=33.4, val_loss=32.40, train_loss=34.60]
    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 253.19it/s, loss=33.4, val_loss=32.40, train_loss=34.60]
    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 251.14it/s, loss=33.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 249.87it/s, loss=33.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 248.13it/s, loss=32.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 248.90it/s, loss=32.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 247.57it/s, loss=30.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 249.96it/s, loss=30.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 248.83it/s, loss=31, val_loss=32.40, train_loss=34.60]  
    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 248.84it/s, loss=31, val_loss=32.40, train_loss=34.60]
    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 247.72it/s, loss=30.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 250.14it/s, loss=30.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 249.20it/s, loss=31.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 251.22it/s, loss=31.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 250.38it/s, loss=30.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 251.35it/s, loss=30.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 250.56it/s, loss=30.9, val_loss=32.40, train_loss=34.60]
    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 252.69it/s, loss=30.9, val_loss=32.40, train_loss=34.60]
    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 251.99it/s, loss=31.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 252.16it/s, loss=31.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 251.36it/s, loss=31.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 251.86it/s, loss=31.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 251.25it/s, loss=32.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 252.68it/s, loss=32.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 252.10it/s, loss=32.5, val_loss=32.40, train_loss=34.60]
    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 253.17it/s, loss=32.5, val_loss=32.40, train_loss=34.60]
    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 252.51it/s, loss=32.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 249.94it/s, loss=32.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 249.34it/s, loss=32.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 246.72it/s, loss=32.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 246.21it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 246.83it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 246.35it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 246.26it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 245.76it/s, loss=35.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 246.60it/s, loss=35.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 246.13it/s, loss=34.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 246.02it/s, loss=34.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 245.50it/s, loss=34.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 246.01it/s, loss=34.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 245.60it/s, loss=34.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 246.14it/s, loss=34.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 245.71it/s, loss=34.9, val_loss=32.40, train_loss=34.60]
    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 246.11it/s, loss=34.9, val_loss=32.40, train_loss=34.60]
    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 245.71it/s, loss=37, val_loss=32.40, train_loss=34.60]  
    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 246.02it/s, loss=37, val_loss=32.40, train_loss=34.60]
    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 245.67it/s, loss=36, val_loss=32.40, train_loss=34.60]
    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 246.24it/s, loss=36, val_loss=32.40, train_loss=34.60]
    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 245.92it/s, loss=34, val_loss=32.40, train_loss=34.60]
    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 247.05it/s, loss=34, val_loss=32.40, train_loss=34.60]
    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 246.68it/s, loss=35.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 246.73it/s, loss=35.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 246.39it/s, loss=34.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 246.85it/s, loss=34.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 246.55it/s, loss=35.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 247.23it/s, loss=35.2, val_loss=32.40, train_loss=34.60]
    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 246.89it/s, loss=35.5, val_loss=32.40, train_loss=34.60]
    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 247.25it/s, loss=35.5, val_loss=32.40, train_loss=34.60]
    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 246.95it/s, loss=36, val_loss=32.40, train_loss=34.60]  
    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 246.82it/s, loss=36, val_loss=32.40, train_loss=34.60]
    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 246.55it/s, loss=35.5, val_loss=32.40, train_loss=34.60]
    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 246.75it/s, loss=35.5, val_loss=32.40, train_loss=34.60]
    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 246.48it/s, loss=35.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 247.43it/s, loss=35.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 247.18it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 247.15it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 246.90it/s, loss=38, val_loss=32.40, train_loss=34.60]  
    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 247.09it/s, loss=38, val_loss=32.40, train_loss=34.60]
    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 246.80it/s, loss=38.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 246.91it/s, loss=38.6, val_loss=32.40, train_loss=34.60]
    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 246.68it/s, loss=37.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 247.40it/s, loss=37.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 247.17it/s, loss=38.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 247.32it/s, loss=38.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 247.02it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 246.84it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 246.62it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 246.96it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 246.72it/s, loss=37, val_loss=32.40, train_loss=34.60]  
    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 246.64it/s, loss=37, val_loss=32.40, train_loss=34.60]
    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 246.39it/s, loss=37.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 246.12it/s, loss=37.1, val_loss=32.40, train_loss=34.60]
    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 245.91it/s, loss=36.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 246.07it/s, loss=36.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 245.86it/s, loss=34.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 246.52it/s, loss=34.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 246.33it/s, loss=36.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 246.62it/s, loss=36.8, val_loss=32.40, train_loss=34.60]
    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 246.36it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 246.33it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 246.13it/s, loss=35.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 246.08it/s, loss=35.3, val_loss=32.40, train_loss=34.60]
    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 245.87it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  81%|████████  | 51/63 [00:00<00:00, 248.33it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  83%|████████▎ | 52/63 [00:00<00:00, 252.37it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  84%|████████▍ | 53/63 [00:00<00:00, 256.40it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  86%|████████▌ | 54/63 [00:00<00:00, 260.47it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  87%|████████▋ | 55/63 [00:00<00:00, 264.50it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  89%|████████▉ | 56/63 [00:00<00:00, 268.52it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  90%|█████████ | 57/63 [00:00<00:00, 272.50it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  92%|█████████▏| 58/63 [00:00<00:00, 276.51it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  94%|█████████▎| 59/63 [00:00<00:00, 280.44it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  95%|█████████▌| 60/63 [00:00<00:00, 284.40it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  97%|█████████▋| 61/63 [00:00<00:00, 288.27it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:  98%|█████████▊| 62/63 [00:00<00:00, 292.05it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 295.83it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 294.46it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 293.85it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=35.7, val_loss=32.40, train_loss=34.60]          
    Epoch 2:   0%|          | 0/63 [00:00<?, ?it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 214.32it/s, loss=35.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 205.89it/s, loss=36, val_loss=32.40, train_loss=34.60]  
    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 215.96it/s, loss=36, val_loss=32.40, train_loss=34.60]
    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 211.98it/s, loss=38, val_loss=32.40, train_loss=34.60]
    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 227.40it/s, loss=38, val_loss=32.40, train_loss=34.60]
    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 224.25it/s, loss=36.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 225.24it/s, loss=36.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 222.98it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 222.33it/s, loss=37.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 220.54it/s, loss=37.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 224.42it/s, loss=37.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 223.18it/s, loss=36.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 228.75it/s, loss=36.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 227.38it/s, loss=33.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 229.18it/s, loss=33.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 228.12it/s, loss=34.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 230.36it/s, loss=34.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 229.27it/s, loss=35.1, val_loss=32.40, train_loss=34.60]
    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 229.07it/s, loss=35.1, val_loss=32.40, train_loss=34.60]
    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 228.02it/s, loss=35, val_loss=32.40, train_loss=34.60]  
    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 228.16it/s, loss=35, val_loss=32.40, train_loss=34.60]
    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 227.32it/s, loss=34.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 228.88it/s, loss=34.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 228.18it/s, loss=35.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 229.20it/s, loss=35.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 228.62it/s, loss=38.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 230.95it/s, loss=38.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 230.41it/s, loss=37.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 232.23it/s, loss=37.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 231.57it/s, loss=38.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 232.90it/s, loss=38.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 232.36it/s, loss=39.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 233.60it/s, loss=39.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 233.11it/s, loss=38.2, val_loss=32.40, train_loss=34.60]
    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 235.66it/s, loss=38.2, val_loss=32.40, train_loss=34.60]
    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 235.22it/s, loss=37.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 236.15it/s, loss=37.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 235.76it/s, loss=37.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 238.02it/s, loss=37.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 237.61it/s, loss=37.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 238.30it/s, loss=37.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 237.90it/s, loss=38.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 238.80it/s, loss=38.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 238.38it/s, loss=35.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 239.00it/s, loss=35.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 238.55it/s, loss=36.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 239.09it/s, loss=36.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 238.74it/s, loss=36.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 238.81it/s, loss=36.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 238.47it/s, loss=37.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 239.40it/s, loss=37.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 239.10it/s, loss=37.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 239.23it/s, loss=37.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 238.89it/s, loss=38.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 239.98it/s, loss=38.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 239.68it/s, loss=36.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 240.23it/s, loss=36.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 239.94it/s, loss=35.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 241.23it/s, loss=35.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 240.97it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 241.21it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 240.91it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 241.37it/s, loss=33.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 241.08it/s, loss=34.1, val_loss=32.40, train_loss=34.60]
    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 241.82it/s, loss=34.1, val_loss=32.40, train_loss=34.60]
    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 241.54it/s, loss=31.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 241.48it/s, loss=31.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 241.16it/s, loss=32.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 241.49it/s, loss=32.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 241.25it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 242.25it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 241.98it/s, loss=33.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 241.96it/s, loss=33.4, val_loss=32.40, train_loss=34.60]
    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 241.68it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 241.59it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 241.32it/s, loss=33.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 241.89it/s, loss=33.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 241.63it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 241.81it/s, loss=33.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 241.63it/s, loss=33.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 242.21it/s, loss=33.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 241.98it/s, loss=30.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 242.08it/s, loss=30.8, val_loss=32.40, train_loss=34.60]
    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 241.86it/s, loss=31.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 242.15it/s, loss=31.3, val_loss=32.40, train_loss=34.60]
    Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 241.93it/s, loss=30.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 242.16it/s, loss=30.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 241.96it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 242.84it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 242.65it/s, loss=30, val_loss=32.40, train_loss=34.60]  
    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 243.12it/s, loss=30, val_loss=32.40, train_loss=34.60]
    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 242.93it/s, loss=29.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 243.41it/s, loss=29.9, val_loss=32.40, train_loss=34.60]
    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 243.21it/s, loss=29.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 243.82it/s, loss=29.6, val_loss=32.40, train_loss=34.60]
    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 243.65it/s, loss=30.1, val_loss=32.40, train_loss=34.60]
    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 243.36it/s, loss=30.1, val_loss=32.40, train_loss=34.60]
    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 243.16it/s, loss=30.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 243.64it/s, loss=30.7, val_loss=32.40, train_loss=34.60]
    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 243.45it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  81%|████████  | 51/63 [00:00<00:00, 246.11it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  83%|████████▎ | 52/63 [00:00<00:00, 250.15it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  84%|████████▍ | 53/63 [00:00<00:00, 254.28it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  86%|████████▌ | 54/63 [00:00<00:00, 258.36it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  87%|████████▋ | 55/63 [00:00<00:00, 262.46it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  89%|████████▉ | 56/63 [00:00<00:00, 266.51it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  90%|█████████ | 57/63 [00:00<00:00, 270.59it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  92%|█████████▏| 58/63 [00:00<00:00, 274.60it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  94%|█████████▎| 59/63 [00:00<00:00, 278.64it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  95%|█████████▌| 60/63 [00:00<00:00, 282.61it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  97%|█████████▋| 61/63 [00:00<00:00, 286.61it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2:  98%|█████████▊| 62/63 [00:00<00:00, 290.55it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 294.54it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 293.29it/s, loss=31.5, val_loss=32.40, train_loss=34.60]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 292.59it/s, loss=31.5, val_loss=32.40, train_loss=34.50]
    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 285.35it/s, loss=31.5, val_loss=32.40, train_loss=34.50]
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         Validate metric           DataLoader 0
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
             MAE_val             4.598403453826904
             R2_val           -0.00040531158447265625
            val_loss             32.35670471191406
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────




.. GENERATED FROM PYTHON SOURCE LINES 185-187

7. Plotting the results of the second model
----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 187-192

.. code-block:: default


    plotter = Plotter(single_model_dict, params)
    single_model_figures_dict = plotter.plot_all()
    plotter.show_all(single_model_figures_dict)




.. image-sg:: /auto_examples/images/sphx_glr_plot_two_models_traintest_002.png
   :alt: TabularChannelWiseMultiAttention - Validation R2: -0.000
   :srcset: /auto_examples/images/sphx_glr_plot_two_models_traintest_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Plotting models ['TabularChannelWiseMultiAttention'] ...
    Plotting results of a single model.
    TabularChannelWiseMultiAttention_reals_vs_preds




.. GENERATED FROM PYTHON SOURCE LINES 193-196

8. Comparing the results of the two models
---------------------------------------------
Now we're going to compare the results of the two models. We're using the same steps as when we used Plotter before, but this time we're using the ``all_trained_models`` dictionary which contains both models.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

.. code-block:: default


    comparison_plotter = Plotter(all_trained_models, params)
    comparison_plot_dict = comparison_plotter.plot_all()
    comparison_plotter.show_all(comparison_plot_dict)




.. image-sg:: /auto_examples/images/sphx_glr_plot_two_models_traintest_003.png
   :alt: Model Performance Comparison, R2, MAE
   :srcset: /auto_examples/images/sphx_glr_plot_two_models_traintest_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Plotting models ['ConcatTabularData', 'TabularChannelWiseMultiAttention'] ...
    Plotting comparison plots for 2 models: ['ConcatTabularData', 'TabularChannelWiseMultiAttention']
    compare_tt_models




.. GENERATED FROM PYTHON SOURCE LINES 202-205

9. Saving the metrics of the two models
-----------------------------------------
We can also get the metrics of the two models into a Pandas DataFrame using the :func:`~fusionlibrary.eval.Plotter.get_performance_df` function.

.. GENERATED FROM PYTHON SOURCE LINES 205-207

.. code-block:: default

    performances_df = comparison_plotter.get_performance_df()
    performances_df





.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>R2</th>
          <th>MAE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>ConcatTabularData</th>
          <td>0.084105</td>
          <td>4.256636</td>
        </tr>
        <tr>
          <th>TabularChannelWiseMultiAttention</th>
          <td>-0.000405</td>
          <td>4.598403</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.290 seconds)


.. _sphx_glr_download_auto_examples_plot_two_models_traintest.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_two_models_traintest.py <plot_two_models_traintest.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_two_models_traintest.ipynb <plot_two_models_traintest.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
