<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Regression: comparing two tabular models trained on simulated data &mdash; fusionlibrary  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/florencestheme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/pink_pasta_logo.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to create your own fusion model" href="Creating%20your%20own%20fusion%20model/index.html" />
    <link rel="prev" title="Binary: training one kfold model" href="plot_one_model_binary_kfold.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            fusionlibrary
              <img src="../_static/pink_pasta_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">üå∏ Table of Contents üå∏</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Fusilli: an introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fusion_model_explanations.html">Fusion Model Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modifying_models.html">Modifying the fusion models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üå∏ Tutorials üå∏</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials and Examples Gallery</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modify_layer_sizes.html">How to modify architectures of fusion models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_one_model_binary_kfold.html">Binary: training one kfold model</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Regression: comparing two tabular models trained on simulated data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#import-fusion-models">1. Import fusion models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#set-the-training-parameters">2. Set the training parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-simulated-data">3. Generating simulated data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-first-fusion-model">4. Training the first fusion model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plotting-the-results-of-the-first-model">5. Plotting the results of the first model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-second-fusion-model">6. Training the second fusion model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plotting-the-results-of-the-second-model">7. Plotting the results of the second model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparing-the-results-of-the-two-models">8. Comparing the results of the two models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#saving-the-metrics-of-the-two-models">9. Saving the metrics of the two models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-to-create-your-own-fusion-model">How to create your own fusion model</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üå∏ API Reference üå∏</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.fusion_models.html">fusionlibrary.fusion_models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.datamodules.html">fusionlibrary.datamodules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.train_functions.html">fusionlibrary.train_functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.eval_functions.Plotter.html">fusionlibrary.eval_functions.Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.utils.model_chooser.html">fusionlibrary.utils.model_chooser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.utils.pl_utils.html">fusionlibrary.utils.pl_utils</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">fusionlibrary</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Tutorials and Examples Gallery</a></li>
      <li class="breadcrumb-item active">Regression: comparing two tabular models trained on simulated data</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/auto_examples/plot_two_models_traintest.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-two-models-traintest-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="regression-comparing-two-tabular-models-trained-on-simulated-data">
<span id="sphx-glr-auto-examples-plot-two-models-traintest-py"></span><h1>Regression: comparing two tabular models trained on simulated data<a class="headerlink" href="#regression-comparing-two-tabular-models-trained-on-simulated-data" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>This script shows how to train two fusion models on a regression task with train/test protocol and multimodal tabular data.</p>
<p>Key Features:</p>
<ul class="simple">
<li><p>Importing models based on name.</p></li>
<li><p>Training and testing models with train/test protocol.</p></li>
<li><p>Saving trained models to a dictionary for later analysis.</p></li>
<li><p>Plotting the results of a single model.</p></li>
<li><p>Plotting the results of multiple models as a bar chart.</p></li>
<li><p>Saving the results of multiple models as a csv file.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">importlib</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">docs.examples</span> <span class="kn">import</span> <span class="n">generate_sklearn_simulated_data</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.datamodules</span> <span class="kn">import</span> <span class="n">get_data_module</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.eval_functions</span> <span class="kn">import</span> <span class="n">Plotter</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.fusion_models.base_pl_model</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.train_functions</span> <span class="kn">import</span> <span class="n">train_and_save_models</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.utils.model_chooser</span> <span class="kn">import</span> <span class="n">get_models</span>
</pre></div>
</div>
<section id="import-fusion-models">
<h2>1. Import fusion models<a class="headerlink" href="#import-fusion-models" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Here we import the fusion models to be compared. The models are imported using the
<a class="reference internal" href="../autosummary/fusionlibrary.utils.model_chooser.html#fusionlibrary.utils.model_chooser.get_models" title="fusionlibrary.utils.model_chooser.get_models"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_models()</span></code></a> function, which takes a dictionary of conditions
as an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.</p>
<p>The function returns a dataframe of the models that match the conditions. The dataframe contains the
method name, the class name, the modality type, the fusion type, the path to the model, and the path to the
model‚Äôs parent class. The paths are used to import the models with the <code class="xref py py-func docutils literal notranslate"><span class="pre">importlib.import_module()</span></code>.</p>
<p>We‚Äôre importing ConcatTabularData and TabularChannelWiseMultiAttention models for this example. Both are multimodal tabular models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_conditions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;class_name&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;ConcatTabularData&quot;</span><span class="p">,</span> <span class="s2">&quot;TabularChannelWiseMultiAttention&quot;</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">imported_models</span> <span class="o">=</span> <span class="n">get_models</span><span class="p">(</span><span class="n">model_conditions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Imported methods:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">imported_models</span><span class="o">.</span><span class="n">method_name</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">fusion_models</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># contains the class objects for each model</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">imported_models</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;method_path&quot;</span><span class="p">])</span>
    <span class="n">module_class</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;class_name&quot;</span><span class="p">])</span>

    <span class="n">fusion_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module_class</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Imported methods:
[&#39;Concatenating tabular data&#39; &#39;Channel-wise multiplication net (tabular)&#39;]
</pre></div>
</div>
</section>
<section id="set-the-training-parameters">
<h2>2. Set the training parameters<a class="headerlink" href="#set-the-training-parameters" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Here we define the parameters for training and testing the models. The parameters are stored in a dictionary and passed to most
of the methods in this library.
For training and testing, the necessary parameters are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_size</span></code>: the proportion of the data to be used for testing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kfold_flag</span></code>: the user sets this to False for train/test protocol.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log</span></code>: a boolean of whether to log the results using Weights and Biases.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pred_type</span></code>: the type of prediction to be performed. This is either <code class="docutils literal notranslate"><span class="pre">regression</span></code>, <code class="docutils literal notranslate"><span class="pre">binary</span></code>, or <code class="docutils literal notranslate"><span class="pre">classification</span></code>. For this example we‚Äôre using regression.</p></li>
</ul>
<p>If we were going to use a subspace-based fusion model, we would also need to set the latent dimensionality of the subspace with <code class="docutils literal notranslate"><span class="pre">subspace_latdims</span></code>. This will be shown in a different example.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;test_size&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s2">&quot;kfold_flag&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;pred_type&quot;</span><span class="p">:</span> <span class="s2">&quot;regression&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="generating-simulated-data">
<h2>3. Generating simulated data<a class="headerlink" href="#generating-simulated-data" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Here we generate simulated data for the two tabular modalities for this example.
This function also simulated image data which we aren‚Äôt using here.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">generate_sklearn_simulated_data</span><span class="p">(</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">num_tab1_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">num_tab2_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">img_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-first-fusion-model">
<h2>4. Training the first fusion model<a class="headerlink" href="#training-the-first-fusion-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Here we train the first fusion model. We‚Äôre using the <code class="docutils literal notranslate"><span class="pre">train_and_save_models</span></code> function to train and test the models.
This function takes the following inputs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">trained_models_dict</span></code>: a dictionary to store the trained models.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_module</span></code>: the data module containing the data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>: the parameters for training and testing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fusion_model</span></code>: the fusion model to be trained.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init_model</span></code>: the initialised dummy fusion model.</p></li>
</ul>
<p>First we‚Äôll create a dictionary to store both the trained models so we can compare them later.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">all_trained_models</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># create dictionary to store trained models</span>
</pre></div>
</div>
<p>To train the first model we need to:</p>
<ol class="arabic simple">
<li><p><em>Choose the model</em>: We‚Äôre using the first model in the <code class="docutils literal notranslate"><span class="pre">fusion_models</span></code> list we made earlier.</p></li>
<li><p><em>Create a dictionary to store the trained model</em>: We‚Äôre using the name of the model as the key. It may seem overkill to make a dictionary just to store one model, but we also use this when we do k-fold training to store the trained models from the different folds.</p></li>
<li><p><em>Initialise the model with dummy data</em>: This is so we can find out whether there are extra instructions for creating the datamodule (such as a method for creating a graph datamodule).</p></li>
<li><p><em>Print the attributes of the model</em>: To check it‚Äôs been initialised correctly.</p></li>
<li><p><em>Create the datamodule</em>: This is done with the <a class="reference internal" href="../autosummary/fusionlibrary.datamodules.html#fusionlibrary.datamodules.get_data_module" title="fusionlibrary.datamodules.get_data_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_data_module()</span></code></a> function. This function takes the initialised model and the parameters as inputs. It returns the datamodule.</p></li>
<li><p><em>Train and test the model</em>: This is done with the <a class="reference internal" href="../autosummary/fusionlibrary.train_functions.html#fusionlibrary.train_functions.train_and_save_models" title="fusionlibrary.train_functions.train_and_save_models"><code class="xref py py-func docutils literal notranslate"><span class="pre">train_and_save_models()</span></code></a> function. This function takes the trained_models_dict, the datamodule, the parameters, the fusion model, and the initialised model as inputs. It returns the trained_models_dict with the trained model added to it.</p></li>
<li><p><em>Add the trained model to the ``all_trained_models`` dictionary</em>: This is so we can compare the results of the two models later.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fusion_model</span> <span class="o">=</span> <span class="n">fusion_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">single_model_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Initialise model</span>
<span class="n">init_model</span> <span class="o">=</span> <span class="n">BaseModel</span><span class="p">(</span>
    <span class="n">fusion_model</span><span class="p">(</span>
        <span class="n">params</span><span class="p">[</span><span class="s2">&quot;pred_type&quot;</span><span class="p">],</span> <span class="n">data_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]],</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Method name:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">method_name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modality type:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">modality_type</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fusion type:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">fusion_type</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Metrics:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">metric_names_list</span><span class="p">)</span>

<span class="c1"># Create the data module</span>
<span class="n">dm</span> <span class="o">=</span> <span class="n">get_data_module</span><span class="p">(</span><span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># Train and test</span>
<span class="n">single_model_dict</span> <span class="o">=</span> <span class="n">train_and_save_models</span><span class="p">(</span>
    <span class="n">trained_models_dict</span><span class="o">=</span><span class="n">single_model_dict</span><span class="p">,</span>
    <span class="n">data_module</span><span class="o">=</span><span class="n">dm</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">fusion_model</span><span class="o">=</span><span class="n">fusion_model</span><span class="p">,</span>
    <span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add trained model to dictionary</span>
<span class="n">all_trained_models</span><span class="p">[</span><span class="n">fusion_model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span> <span class="o">=</span> <span class="n">single_model_dict</span><span class="p">[</span><span class="n">fusion_model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Method name: Concatenating tabular data
Modality type: both_tab
Fusion type: operation
Metrics: [&#39;R2&#39;, &#39;MAE&#39;]

Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/63 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/63 [00:00&lt;?, ?it/s]
Epoch 0:   2%|‚ñè         | 1/63 [00:00&lt;00:02, 21.79it/s]
Epoch 0:   2%|‚ñè         | 1/63 [00:00&lt;00:02, 21.41it/s, loss=65.7]
Epoch 0:   3%|‚ñé         | 2/63 [00:00&lt;00:01, 40.42it/s, loss=65.7]
Epoch 0:   3%|‚ñé         | 2/63 [00:00&lt;00:01, 40.27it/s, loss=47.8]
Epoch 0:   5%|‚ñç         | 3/63 [00:00&lt;00:01, 58.03it/s, loss=47.8]
Epoch 0:   5%|‚ñç         | 3/63 [00:00&lt;00:01, 57.88it/s, loss=42]
Epoch 0:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 74.66it/s, loss=42]
Epoch 0:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 74.37it/s, loss=41]
Epoch 0:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 89.29it/s, loss=41]
Epoch 0:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 89.04it/s, loss=37.5]
Epoch 0:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 102.89it/s, loss=37.5]
Epoch 0:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 102.57it/s, loss=35.1]
Epoch 0:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 115.72it/s, loss=35.1]
Epoch 0:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 115.41it/s, loss=33.1]
Epoch 0:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 127.49it/s, loss=33.1]
Epoch 0:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 127.15it/s, loss=32.1]
Epoch 0:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 138.73it/s, loss=32.1]
Epoch 0:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 138.40it/s, loss=32.7]
Epoch 0:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 149.54it/s, loss=32.7]
Epoch 0:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 149.22it/s, loss=30.8]
Epoch 0:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 159.37it/s, loss=30.8]
Epoch 0:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 159.08it/s, loss=29.4]
Epoch 0:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 169.17it/s, loss=29.4]
Epoch 0:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 168.79it/s, loss=29.1]
Epoch 0:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 178.39it/s, loss=29.1]
Epoch 0:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 177.95it/s, loss=29.6]
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 186.70it/s, loss=29.6]
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 186.36it/s, loss=28.9]
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 195.20it/s, loss=28.9]
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 194.71it/s, loss=28.6]
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 202.34it/s, loss=28.6]
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 201.95it/s, loss=29.8]
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 210.04it/s, loss=29.8]
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 209.74it/s, loss=29.4]
Epoch 0:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 217.41it/s, loss=29.4]
Epoch 0:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 216.92it/s, loss=30.3]
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 223.19it/s, loss=30.3]
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 222.79it/s, loss=30.7]
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 229.91it/s, loss=30.7]
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 229.58it/s, loss=30.2]
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 236.31it/s, loss=30.2]
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 235.90it/s, loss=29.1]
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 242.23it/s, loss=29.1]
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 241.93it/s, loss=29.7]
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 248.20it/s, loss=29.7]
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 247.71it/s, loss=29]
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 252.75it/s, loss=29]
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 252.36it/s, loss=29.6]
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 258.08it/s, loss=29.6]
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 257.68it/s, loss=29.7]
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 263.29it/s, loss=29.7]
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 262.93it/s, loss=31.8]
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 268.39it/s, loss=31.8]
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 267.95it/s, loss=32.1]
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 272.27it/s, loss=32.1]
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 271.92it/s, loss=33.4]
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 276.79it/s, loss=33.4]
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 276.39it/s, loss=33.8]
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 281.20it/s, loss=33.8]
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 280.85it/s, loss=33.9]
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 285.89it/s, loss=33.9]
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 285.51it/s, loss=34.3]
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 290.15it/s, loss=34.3]
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 289.81it/s, loss=33.9]
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 294.29it/s, loss=33.9]
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 293.82it/s, loss=32.4]
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 296.18it/s, loss=32.4]
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 295.81it/s, loss=33.4]
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 299.89it/s, loss=33.4]
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 299.55it/s, loss=32.7]
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 303.80it/s, loss=32.7]
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 303.36it/s, loss=32.9]
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 306.60it/s, loss=32.9]
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 306.06it/s, loss=32]
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 309.59it/s, loss=32]
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 309.21it/s, loss=30.9]
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 312.80it/s, loss=30.9]
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 312.34it/s, loss=29.5]
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 315.54it/s, loss=29.5]
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 315.19it/s, loss=30.2]
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 318.48it/s, loss=30.2]
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 317.93it/s, loss=29.1]
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 321.49it/s, loss=29.1]
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 321.22it/s, loss=28]
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 324.90it/s, loss=28]
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 324.47it/s, loss=28.2]
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 326.96it/s, loss=28.2]
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 326.44it/s, loss=27.4]
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 329.57it/s, loss=27.4]
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 329.19it/s, loss=27.2]
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 331.88it/s, loss=27.2]
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 331.54it/s, loss=24.7]
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 334.48it/s, loss=24.7]
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 334.07it/s, loss=23.5]
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 336.79it/s, loss=23.5]
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 336.47it/s, loss=21.6]
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 339.37it/s, loss=21.6]
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 339.03it/s, loss=20.9]
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 341.71it/s, loss=20.9]
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 341.35it/s, loss=21.2]
Epoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:00&lt;00:00, 344.15it/s, loss=21.2]
Epoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:00&lt;00:00, 349.60it/s, loss=21.2]
Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:00&lt;00:00, 355.26it/s, loss=21.2]
Epoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:00&lt;00:00, 360.88it/s, loss=21.2]
Epoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:00&lt;00:00, 366.51it/s, loss=21.2]
Epoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:00&lt;00:00, 372.14it/s, loss=21.2]
Epoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:00&lt;00:00, 377.69it/s, loss=21.2]
Epoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:00&lt;00:00, 383.19it/s, loss=21.2]
Epoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:00&lt;00:00, 388.68it/s, loss=21.2]
Epoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:00&lt;00:00, 394.12it/s, loss=21.2]
Epoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:00&lt;00:00, 399.54it/s, loss=21.2]
Epoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:00&lt;00:00, 405.00it/s, loss=21.2]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 410.47it/s, loss=21.2]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 406.07it/s, loss=21.2, val_loss=28.00]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 401.32it/s, loss=21.2, val_loss=28.00, train_loss=28.20]
Epoch 0:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=21.2, val_loss=28.00, train_loss=28.20]
Epoch 1:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=21.2, val_loss=28.00, train_loss=28.20]
Epoch 1:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 446.82it/s, loss=21.2, val_loss=28.00, train_loss=28.20]
Epoch 1:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 416.47it/s, loss=20.9, val_loss=28.00, train_loss=28.20]
Epoch 1:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 452.80it/s, loss=20.9, val_loss=28.00, train_loss=28.20]
Epoch 1:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 433.18it/s, loss=21.1, val_loss=28.00, train_loss=28.20]
Epoch 1:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 462.45it/s, loss=21.1, val_loss=28.00, train_loss=28.20]
Epoch 1:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 451.21it/s, loss=21.7, val_loss=28.00, train_loss=28.20]
Epoch 1:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 463.83it/s, loss=21.7, val_loss=28.00, train_loss=28.20]
Epoch 1:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 455.89it/s, loss=21.8, val_loss=28.00, train_loss=28.20]
Epoch 1:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 479.12it/s, loss=21.8, val_loss=28.00, train_loss=28.20]
Epoch 1:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 471.51it/s, loss=23, val_loss=28.00, train_loss=28.20]
Epoch 1:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 474.16it/s, loss=23, val_loss=28.00, train_loss=28.20]
Epoch 1:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 468.24it/s, loss=21.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 480.28it/s, loss=21.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 475.71it/s, loss=24.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 486.86it/s, loss=24.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 481.99it/s, loss=24.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 483.51it/s, loss=24.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 480.13it/s, loss=25.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 493.34it/s, loss=25.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 490.49it/s, loss=24.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 498.69it/s, loss=24.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 494.30it/s, loss=25.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 494.42it/s, loss=25.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 491.64it/s, loss=26.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 499.86it/s, loss=26.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 497.26it/s, loss=26.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 503.58it/s, loss=26.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 501.26it/s, loss=25.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 500.43it/s, loss=25.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 497.71it/s, loss=26, val_loss=28.00, train_loss=28.20]
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 494.79it/s, loss=26, val_loss=28.00, train_loss=28.20]
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 492.31it/s, loss=26.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 495.18it/s, loss=26.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 492.98it/s, loss=27.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 497.32it/s, loss=27.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 495.36it/s, loss=28.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 497.51it/s, loss=28.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 495.33it/s, loss=29.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 490.68it/s, loss=29.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 488.78it/s, loss=30.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 492.04it/s, loss=30.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 490.58it/s, loss=31.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 494.42it/s, loss=31.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 492.32it/s, loss=31.4, val_loss=28.00, train_loss=28.20]
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 491.55it/s, loss=31.4, val_loss=28.00, train_loss=28.20]
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 489.98it/s, loss=31.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 492.58it/s, loss=31.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 491.24it/s, loss=32, val_loss=28.00, train_loss=28.20]
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 494.29it/s, loss=32, val_loss=28.00, train_loss=28.20]
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 493.09it/s, loss=31.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 497.88it/s, loss=31.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 496.72it/s, loss=32.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 501.39it/s, loss=32.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 500.33it/s, loss=30.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 502.76it/s, loss=30.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 500.75it/s, loss=29.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 502.54it/s, loss=29.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 501.00it/s, loss=29.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 500.28it/s, loss=29.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 499.10it/s, loss=29.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 500.83it/s, loss=29.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 499.68it/s, loss=28.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 503.07it/s, loss=28.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 502.08it/s, loss=28.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 504.97it/s, loss=28.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 503.59it/s, loss=28.4, val_loss=28.00, train_loss=28.20]
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 503.35it/s, loss=28.4, val_loss=28.00, train_loss=28.20]
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 502.02it/s, loss=28.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 503.00it/s, loss=28.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 501.98it/s, loss=28.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 502.86it/s, loss=28.3, val_loss=28.00, train_loss=28.20]
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 501.89it/s, loss=28, val_loss=28.00, train_loss=28.20]
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 501.95it/s, loss=28, val_loss=28.00, train_loss=28.20]
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 500.75it/s, loss=27.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 501.67it/s, loss=27.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 500.65it/s, loss=27, val_loss=28.00, train_loss=28.20]
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 502.45it/s, loss=27, val_loss=28.00, train_loss=28.20]
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 501.52it/s, loss=26.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 502.67it/s, loss=26.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 501.74it/s, loss=25.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 503.31it/s, loss=25.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 502.41it/s, loss=26.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 504.16it/s, loss=26.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 503.18it/s, loss=26.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 503.74it/s, loss=26.9, val_loss=28.00, train_loss=28.20]
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 502.70it/s, loss=26.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 503.15it/s, loss=26.6, val_loss=28.00, train_loss=28.20]
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 502.36it/s, loss=25.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 503.83it/s, loss=25.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 502.79it/s, loss=25.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 502.82it/s, loss=25.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 501.94it/s, loss=25.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 503.20it/s, loss=25.5, val_loss=28.00, train_loss=28.20]
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 502.47it/s, loss=24.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 504.04it/s, loss=24.7, val_loss=28.00, train_loss=28.20]
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 503.05it/s, loss=25.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 503.14it/s, loss=25.2, val_loss=28.00, train_loss=28.20]
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 502.43it/s, loss=24.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 504.69it/s, loss=24.8, val_loss=28.00, train_loss=28.20]
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 504.11it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:00&lt;00:00, 506.98it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:00&lt;00:00, 514.67it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:00&lt;00:00, 522.39it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:00&lt;00:00, 530.06it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:00&lt;00:00, 537.71it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:00&lt;00:00, 545.32it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:00&lt;00:00, 552.87it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:00&lt;00:00, 560.37it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:00&lt;00:00, 567.78it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:00&lt;00:00, 575.19it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:00&lt;00:00, 582.54it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:00&lt;00:00, 589.85it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 597.17it/s, loss=26.1, val_loss=28.00, train_loss=28.20]
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 592.71it/s, loss=26.1, val_loss=27.80, train_loss=28.20]
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 590.52it/s, loss=26.1, val_loss=27.80, train_loss=28.00]
Epoch 1:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=26.1, val_loss=27.80, train_loss=28.00]
Epoch 2:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=26.1, val_loss=27.80, train_loss=28.00]
Epoch 2:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 447.44it/s, loss=26.1, val_loss=27.80, train_loss=28.00]
Epoch 2:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 420.48it/s, loss=27.2, val_loss=27.80, train_loss=28.00]
Epoch 2:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 487.45it/s, loss=27.2, val_loss=27.80, train_loss=28.00]
Epoch 2:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 467.83it/s, loss=26.8, val_loss=27.80, train_loss=28.00]
Epoch 2:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 481.55it/s, loss=26.8, val_loss=27.80, train_loss=28.00]
Epoch 2:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 470.97it/s, loss=26.6, val_loss=27.80, train_loss=28.00]
Epoch 2:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 488.05it/s, loss=26.6, val_loss=27.80, train_loss=28.00]
Epoch 2:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 479.21it/s, loss=26.5, val_loss=27.80, train_loss=28.00]
Epoch 2:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 488.81it/s, loss=26.5, val_loss=27.80, train_loss=28.00]
Epoch 2:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 480.85it/s, loss=26.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 489.27it/s, loss=26.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 483.83it/s, loss=25.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 498.65it/s, loss=25.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 493.07it/s, loss=26.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 495.02it/s, loss=26.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 490.40it/s, loss=26.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 497.05it/s, loss=26.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 493.20it/s, loss=26.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 500.53it/s, loss=26.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 496.83it/s, loss=26.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 505.81it/s, loss=26.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 503.13it/s, loss=25.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 514.14it/s, loss=25.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 511.68it/s, loss=26.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 518.92it/s, loss=26.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 516.57it/s, loss=26.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 523.44it/s, loss=26.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 519.63it/s, loss=25.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 516.94it/s, loss=25.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 513.93it/s, loss=27.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 516.18it/s, loss=27.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 513.66it/s, loss=27.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 517.52it/s, loss=27.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 515.35it/s, loss=29.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 518.27it/s, loss=29.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 516.07it/s, loss=29.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 519.87it/s, loss=29.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 518.16it/s, loss=30.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 523.04it/s, loss=30.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 521.05it/s, loss=30.5, val_loss=27.80, train_loss=28.00]
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 521.45it/s, loss=30.5, val_loss=27.80, train_loss=28.00]
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 519.31it/s, loss=29.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 519.93it/s, loss=29.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 518.05it/s, loss=28.5, val_loss=27.80, train_loss=28.00]
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 520.51it/s, loss=28.5, val_loss=27.80, train_loss=28.00]
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 519.13it/s, loss=28.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 524.00it/s, loss=28.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 522.64it/s, loss=29.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 524.40it/s, loss=29.3, val_loss=27.80, train_loss=28.00]
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 522.41it/s, loss=30.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 522.16it/s, loss=30.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 520.77it/s, loss=30.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 523.46it/s, loss=30.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 521.79it/s, loss=30.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 523.20it/s, loss=30.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 521.78it/s, loss=31.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 523.17it/s, loss=31.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 521.72it/s, loss=32.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 522.87it/s, loss=32.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 521.31it/s, loss=31.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 522.10it/s, loss=31.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 520.90it/s, loss=29.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 522.71it/s, loss=29.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 521.50it/s, loss=29.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 523.52it/s, loss=29.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 522.32it/s, loss=29.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 524.03it/s, loss=29.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 523.02it/s, loss=29.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 524.00it/s, loss=29.2, val_loss=27.80, train_loss=28.00]
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 522.71it/s, loss=28.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 523.57it/s, loss=28.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 522.47it/s, loss=27.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 524.18it/s, loss=27.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 523.20it/s, loss=26.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 525.81it/s, loss=26.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 524.89it/s, loss=26.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 525.10it/s, loss=26.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 524.02it/s, loss=25.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 525.65it/s, loss=25.8, val_loss=27.80, train_loss=28.00]
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 524.82it/s, loss=24.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 526.61it/s, loss=24.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 525.32it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 524.18it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 523.25it/s, loss=25, val_loss=27.80, train_loss=28.00]
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 524.03it/s, loss=25, val_loss=27.80, train_loss=28.00]
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 523.19it/s, loss=24.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 524.03it/s, loss=24.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 523.09it/s, loss=24.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 523.35it/s, loss=24.9, val_loss=27.80, train_loss=28.00]
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 522.45it/s, loss=23.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 524.29it/s, loss=23.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 523.61it/s, loss=25.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 524.55it/s, loss=25.4, val_loss=27.80, train_loss=28.00]
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 523.63it/s, loss=24.7, val_loss=27.80, train_loss=28.00]
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 524.31it/s, loss=24.7, val_loss=27.80, train_loss=28.00]
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 523.56it/s, loss=24.7, val_loss=27.80, train_loss=28.00]
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 524.71it/s, loss=24.7, val_loss=27.80, train_loss=28.00]
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 524.00it/s, loss=24.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 525.43it/s, loss=24.6, val_loss=27.80, train_loss=28.00]
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 524.69it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:00&lt;00:00, 526.95it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:00&lt;00:00, 534.83it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:00&lt;00:00, 542.60it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:00&lt;00:00, 550.15it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:00&lt;00:00, 557.77it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:00&lt;00:00, 565.34it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:00&lt;00:00, 572.76it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:00&lt;00:00, 580.15it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:00&lt;00:00, 587.47it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:00&lt;00:00, 594.73it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:00&lt;00:00, 599.44it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:00&lt;00:00, 605.31it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 612.07it/s, loss=25.1, val_loss=27.80, train_loss=28.00]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 606.53it/s, loss=25.1, val_loss=26.80, train_loss=28.00]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 602.98it/s, loss=25.1, val_loss=26.80, train_loss=27.50]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 582.33it/s, loss=25.1, val_loss=26.80, train_loss=27.50]
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     Validate metric           DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
         MAE_val             4.126473426818848
         R2_val             0.03733396530151367
        val_loss            26.750049591064453
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
</pre></div>
</div>
</section>
<section id="plotting-the-results-of-the-first-model">
<h2>5. Plotting the results of the first model<a class="headerlink" href="#plotting-the-results-of-the-first-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>We‚Äôre using the <a class="reference internal" href="../autosummary/fusionlibrary.eval_functions.Plotter.html#fusionlibrary.eval_functions.Plotter" title="fusionlibrary.eval_functions.Plotter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Plotter</span></code></a> class to plot the results of the first model. This class takes the dictionary of trained models and the parameters as inputs. It returns a dictionary of figures.
If there is one model in the dictionary (i.e. only one unique key), then it plots the figures for analysing the results of a single model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plotter</span> <span class="o">=</span> <span class="n">Plotter</span><span class="p">(</span><span class="n">single_model_dict</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">single_model_figures_dict</span> <span class="o">=</span> <span class="n">plotter</span><span class="o">.</span><span class="n">plot_all</span><span class="p">()</span>
<span class="n">plotter</span><span class="o">.</span><span class="n">show_all</span><span class="p">(</span><span class="n">single_model_figures_dict</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_two_models_traintest_001.png" srcset="../_images/sphx_glr_plot_two_models_traintest_001.png" alt="ConcatTabularData - Validation R2: 0.037" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Plotting models [&#39;ConcatTabularData&#39;] ...
Plotting results of a single model.
ConcatTabularData_reals_vs_preds
</pre></div>
</div>
</section>
<section id="training-the-second-fusion-model">
<h2>6. Training the second fusion model<a class="headerlink" href="#training-the-second-fusion-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Here we train the second fusion model: TabularChannelWiseMultiAttention. We‚Äôre using the same steps as before, but this time we‚Äôre using the second model in the <code class="docutils literal notranslate"><span class="pre">fusion_models</span></code> list.</p>
<p>Choose the model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fusion_model</span> <span class="o">=</span> <span class="n">fusion_models</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">single_model_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Initialise model</span>
<span class="n">init_model</span> <span class="o">=</span> <span class="n">BaseModel</span><span class="p">(</span>
    <span class="n">fusion_model</span><span class="p">(</span>
        <span class="n">params</span><span class="p">[</span><span class="s2">&quot;pred_type&quot;</span><span class="p">],</span> <span class="n">data_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]],</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Method name:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">method_name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modality type:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">modality_type</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fusion type:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">fusion_type</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Metrics:&quot;</span><span class="p">,</span> <span class="n">init_model</span><span class="o">.</span><span class="n">metric_names_list</span><span class="p">)</span>

<span class="c1"># Create the data module</span>
<span class="n">dm</span> <span class="o">=</span> <span class="n">get_data_module</span><span class="p">(</span><span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># Train and test</span>
<span class="n">trained_models</span> <span class="o">=</span> <span class="n">train_and_save_models</span><span class="p">(</span>
    <span class="n">trained_models_dict</span><span class="o">=</span><span class="n">single_model_dict</span><span class="p">,</span>
    <span class="n">data_module</span><span class="o">=</span><span class="n">dm</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">fusion_model</span><span class="o">=</span><span class="n">fusion_model</span><span class="p">,</span>
    <span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add trained model to dictionary</span>
<span class="n">all_trained_models</span><span class="p">[</span><span class="n">fusion_model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span> <span class="o">=</span> <span class="n">single_model_dict</span><span class="p">[</span><span class="n">fusion_model</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Method name: Channel-wise multiplication net (tabular)
Modality type: both_tab
Fusion type: attention
Metrics: [&#39;R2&#39;, &#39;MAE&#39;]

Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/63 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/63 [00:00&lt;?, ?it/s]
Epoch 0:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 95.62it/s]
Epoch 0:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 93.66it/s, loss=27.4]
Epoch 0:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 128.60it/s, loss=27.4]
Epoch 0:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 126.89it/s, loss=21.2]
Epoch 0:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 142.71it/s, loss=21.2]
Epoch 0:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 141.03it/s, loss=27.3]
Epoch 0:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 153.58it/s, loss=27.3]
Epoch 0:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 152.50it/s, loss=30.4]
Epoch 0:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 161.44it/s, loss=30.4]
Epoch 0:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 160.46it/s, loss=33.7]
Epoch 0:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 167.50it/s, loss=33.7]
Epoch 0:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 166.57it/s, loss=30.9]
Epoch 0:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 172.78it/s, loss=30.9]
Epoch 0:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 171.98it/s, loss=31]
Epoch 0:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 178.25it/s, loss=31]
Epoch 0:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 177.45it/s, loss=32.2]
Epoch 0:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 180.89it/s, loss=32.2]
Epoch 0:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 180.26it/s, loss=30.4]
Epoch 0:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 183.99it/s, loss=30.4]
Epoch 0:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 183.37it/s, loss=30.7]
Epoch 0:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 183.14it/s, loss=30.7]
Epoch 0:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 182.71it/s, loss=33.5]
Epoch 0:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 186.88it/s, loss=33.5]
Epoch 0:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 186.33it/s, loss=33.6]
Epoch 0:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 187.72it/s, loss=33.6]
Epoch 0:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 187.32it/s, loss=32.7]
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 190.30it/s, loss=32.7]
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 189.82it/s, loss=31.6]
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 192.19it/s, loss=31.6]
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 191.76it/s, loss=31.3]
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 193.48it/s, loss=31.3]
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 193.08it/s, loss=30.7]
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 194.86it/s, loss=30.7]
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 194.48it/s, loss=29.8]
Epoch 0:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 195.88it/s, loss=29.8]
Epoch 0:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 195.52it/s, loss=29.1]
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 197.92it/s, loss=29.1]
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 197.58it/s, loss=28.9]
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 200.15it/s, loss=28.9]
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 199.80it/s, loss=29.5]
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 201.46it/s, loss=29.5]
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 201.17it/s, loss=29.5]
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 203.90it/s, loss=29.5]
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 203.67it/s, loss=30.3]
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 206.20it/s, loss=30.3]
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 205.86it/s, loss=29.4]
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 207.07it/s, loss=29.4]
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 206.79it/s, loss=27.9]
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 208.33it/s, loss=27.9]
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 208.04it/s, loss=26.3]
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 209.84it/s, loss=26.3]
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 209.62it/s, loss=26.5]
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 211.84it/s, loss=26.5]
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 211.52it/s, loss=25.9]
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 212.65it/s, loss=25.9]
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 212.40it/s, loss=24.4]
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 214.35it/s, loss=24.4]
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 214.09it/s, loss=25.2]
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 214.98it/s, loss=25.2]
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 214.71it/s, loss=24.8]
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 215.79it/s, loss=24.8]
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 215.52it/s, loss=23.8]
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 216.60it/s, loss=23.8]
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 216.39it/s, loss=22.7]
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 218.07it/s, loss=22.7]
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 217.85it/s, loss=23.4]
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 219.50it/s, loss=23.4]
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 219.29it/s, loss=24]
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 220.70it/s, loss=24]
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 220.52it/s, loss=23.2]
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 222.16it/s, loss=23.2]
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 221.91it/s, loss=23.6]
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 222.53it/s, loss=23.6]
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 222.35it/s, loss=23.8]
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 223.50it/s, loss=23.8]
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 223.26it/s, loss=24.5]
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 224.01it/s, loss=24.5]
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 223.84it/s, loss=25]
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 224.88it/s, loss=25]
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 224.66it/s, loss=24.9]
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 225.29it/s, loss=24.9]
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 225.09it/s, loss=25.3]
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 225.92it/s, loss=25.3]
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 225.72it/s, loss=24.4]
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 226.28it/s, loss=24.4]
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 226.08it/s, loss=24.7]
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 226.78it/s, loss=24.7]
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 226.53it/s, loss=26.1]
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 226.91it/s, loss=26.1]
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 226.72it/s, loss=26]
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 227.33it/s, loss=26]
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 227.15it/s, loss=26.5]
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 227.60it/s, loss=26.5]
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 227.38it/s, loss=27.4]
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 227.70it/s, loss=27.4]
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 227.53it/s, loss=27.3]
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 228.27it/s, loss=27.3]
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 228.14it/s, loss=27.9]
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 229.17it/s, loss=27.9]
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 228.98it/s, loss=27.6]
Epoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:00&lt;00:00, 231.68it/s, loss=27.6]
Epoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:00&lt;00:00, 235.42it/s, loss=27.6]
Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:00&lt;00:00, 239.24it/s, loss=27.6]
Epoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:00&lt;00:00, 243.05it/s, loss=27.6]
Epoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:00&lt;00:00, 246.84it/s, loss=27.6]
Epoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:00&lt;00:00, 250.57it/s, loss=27.6]
Epoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:00&lt;00:00, 254.29it/s, loss=27.6]
Epoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:00&lt;00:00, 258.00it/s, loss=27.6]
Epoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:00&lt;00:00, 261.69it/s, loss=27.6]
Epoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:00&lt;00:00, 265.32it/s, loss=27.6]
Epoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:00&lt;00:00, 268.98it/s, loss=27.6]
Epoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:00&lt;00:00, 272.57it/s, loss=27.6]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 276.17it/s, loss=27.6]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 273.86it/s, loss=27.6, val_loss=32.40]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 273.34it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 0:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 229.94it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 220.31it/s, loss=26.4, val_loss=32.40, train_loss=27.10]
Epoch 1:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 229.41it/s, loss=26.4, val_loss=32.40, train_loss=27.10]
Epoch 1:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 224.19it/s, loss=27, val_loss=32.40, train_loss=27.10]
Epoch 1:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 228.01it/s, loss=27, val_loss=32.40, train_loss=27.10]
Epoch 1:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 224.72it/s, loss=26.1, val_loss=32.40, train_loss=27.10]
Epoch 1:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 231.68it/s, loss=26.1, val_loss=32.40, train_loss=27.10]
Epoch 1:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 229.36it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 1:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 236.10it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 1:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 234.21it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 239.71it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 238.11it/s, loss=27.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 239.30it/s, loss=27.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 237.88it/s, loss=27, val_loss=32.40, train_loss=27.10]
Epoch 1:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 241.18it/s, loss=27, val_loss=32.40, train_loss=27.10]
Epoch 1:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 239.94it/s, loss=26.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 240.07it/s, loss=26.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 238.96it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 238.77it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 237.74it/s, loss=26.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 239.67it/s, loss=26.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 238.84it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 240.44it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 239.48it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 240.28it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 239.50it/s, loss=25.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 240.58it/s, loss=25.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 240.03it/s, loss=25.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 240.62it/s, loss=25.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 239.80it/s, loss=25.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 239.18it/s, loss=25.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 238.59it/s, loss=25.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 240.46it/s, loss=25.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 239.94it/s, loss=24.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 241.54it/s, loss=24.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 241.04it/s, loss=25.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 241.80it/s, loss=25.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 241.28it/s, loss=25.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 242.32it/s, loss=25.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 241.82it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 243.12it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 242.69it/s, loss=27.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 242.42it/s, loss=27.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 241.88it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 241.78it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 241.34it/s, loss=28.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 242.09it/s, loss=28.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 241.64it/s, loss=27.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 242.15it/s, loss=27.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 241.76it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 242.16it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 241.78it/s, loss=27.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 242.49it/s, loss=27.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 242.15it/s, loss=28, val_loss=32.40, train_loss=27.10]
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 243.18it/s, loss=28, val_loss=32.40, train_loss=27.10]
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 242.90it/s, loss=27.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 243.61it/s, loss=27.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 243.22it/s, loss=27.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 242.75it/s, loss=27.9, val_loss=32.40, train_loss=27.10]
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 242.48it/s, loss=28.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 243.46it/s, loss=28.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 243.20it/s, loss=28.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 243.89it/s, loss=28.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 243.47it/s, loss=30.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 243.90it/s, loss=30.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 243.65it/s, loss=29.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 244.35it/s, loss=29.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 244.12it/s, loss=28.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 244.65it/s, loss=28.7, val_loss=32.40, train_loss=27.10]
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 244.29it/s, loss=29.8, val_loss=32.40, train_loss=27.10]
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 244.61it/s, loss=29.8, val_loss=32.40, train_loss=27.10]
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 244.31it/s, loss=30, val_loss=32.40, train_loss=27.10]
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 245.09it/s, loss=30, val_loss=32.40, train_loss=27.10]
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 244.78it/s, loss=30.4, val_loss=32.40, train_loss=27.10]
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 244.86it/s, loss=30.4, val_loss=32.40, train_loss=27.10]
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 244.55it/s, loss=29.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 245.01it/s, loss=29.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 244.78it/s, loss=28.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 245.21it/s, loss=28.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 244.93it/s, loss=26.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 245.41it/s, loss=26.1, val_loss=32.40, train_loss=27.10]
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 245.22it/s, loss=26.2, val_loss=32.40, train_loss=27.10]
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 245.36it/s, loss=26.2, val_loss=32.40, train_loss=27.10]
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 245.08it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 244.95it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 244.70it/s, loss=26.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 245.11it/s, loss=26.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 244.84it/s, loss=26.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 244.89it/s, loss=26.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 244.69it/s, loss=27.8, val_loss=32.40, train_loss=27.10]
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 245.22it/s, loss=27.8, val_loss=32.40, train_loss=27.10]
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 245.06it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 245.81it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 245.56it/s, loss=27.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 245.60it/s, loss=27.5, val_loss=32.40, train_loss=27.10]
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 245.40it/s, loss=28.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 246.19it/s, loss=28.3, val_loss=32.40, train_loss=27.10]
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 245.99it/s, loss=28.4, val_loss=32.40, train_loss=27.10]
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 246.46it/s, loss=28.4, val_loss=32.40, train_loss=27.10]
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 246.30it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:00&lt;00:00, 249.20it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:00&lt;00:00, 253.33it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:00&lt;00:00, 257.49it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:00&lt;00:00, 261.60it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:00&lt;00:00, 265.71it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:00&lt;00:00, 269.79it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:00&lt;00:00, 273.85it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:00&lt;00:00, 277.87it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:00&lt;00:00, 281.89it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:00&lt;00:00, 285.90it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:00&lt;00:00, 289.88it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:00&lt;00:00, 293.84it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 297.81it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 296.63it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 295.83it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 1:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 2:   0%|          | 0/63 [00:00&lt;?, ?it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 2:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 241.41it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 2:   2%|‚ñè         | 1/63 [00:00&lt;00:00, 231.42it/s, loss=25.9, val_loss=32.40, train_loss=27.10]
Epoch 2:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 239.52it/s, loss=25.9, val_loss=32.40, train_loss=27.10]
Epoch 2:   3%|‚ñé         | 2/63 [00:00&lt;00:00, 234.55it/s, loss=24.7, val_loss=32.40, train_loss=27.10]
Epoch 2:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 241.35it/s, loss=24.7, val_loss=32.40, train_loss=27.10]
Epoch 2:   5%|‚ñç         | 3/63 [00:00&lt;00:00, 238.46it/s, loss=25.9, val_loss=32.40, train_loss=27.10]
Epoch 2:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 247.11it/s, loss=25.9, val_loss=32.40, train_loss=27.10]
Epoch 2:   6%|‚ñã         | 4/63 [00:00&lt;00:00, 244.32it/s, loss=26.2, val_loss=32.40, train_loss=27.10]
Epoch 2:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 246.88it/s, loss=26.2, val_loss=32.40, train_loss=27.10]
Epoch 2:   8%|‚ñä         | 5/63 [00:00&lt;00:00, 244.82it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 243.71it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  10%|‚ñâ         | 6/63 [00:00&lt;00:00, 241.95it/s, loss=25, val_loss=32.40, train_loss=27.10]
Epoch 2:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 245.55it/s, loss=25, val_loss=32.40, train_loss=27.10]
Epoch 2:  11%|‚ñà         | 7/63 [00:00&lt;00:00, 243.74it/s, loss=24.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 248.02it/s, loss=24.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  13%|‚ñà‚ñé        | 8/63 [00:00&lt;00:00, 246.81it/s, loss=26.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 248.90it/s, loss=26.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  14%|‚ñà‚ñç        | 9/63 [00:00&lt;00:00, 247.80it/s, loss=27.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 237.91it/s, loss=27.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  16%|‚ñà‚ñå        | 10/63 [00:00&lt;00:00, 236.89it/s, loss=28.5, val_loss=32.40, train_loss=27.10]
Epoch 2:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 237.61it/s, loss=28.5, val_loss=32.40, train_loss=27.10]
Epoch 2:  17%|‚ñà‚ñã        | 11/63 [00:00&lt;00:00, 236.71it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 239.23it/s, loss=27.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  19%|‚ñà‚ñâ        | 12/63 [00:00&lt;00:00, 238.32it/s, loss=25.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 238.04it/s, loss=25.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  21%|‚ñà‚ñà        | 13/63 [00:00&lt;00:00, 237.11it/s, loss=25.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 235.12it/s, loss=25.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 14/63 [00:00&lt;00:00, 234.29it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 234.96it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 15/63 [00:00&lt;00:00, 234.22it/s, loss=24.8, val_loss=32.40, train_loss=27.10]
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 235.41it/s, loss=24.8, val_loss=32.40, train_loss=27.10]
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 16/63 [00:00&lt;00:00, 234.79it/s, loss=25.9, val_loss=32.40, train_loss=27.10]
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 235.56it/s, loss=25.9, val_loss=32.40, train_loss=27.10]
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 17/63 [00:00&lt;00:00, 234.92it/s, loss=25.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 235.19it/s, loss=25.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  29%|‚ñà‚ñà‚ñä       | 18/63 [00:00&lt;00:00, 234.67it/s, loss=23.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 235.78it/s, loss=23.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 19/63 [00:00&lt;00:00, 235.29it/s, loss=23.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 236.07it/s, loss=23.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:00&lt;00:00, 235.50it/s, loss=24, val_loss=32.40, train_loss=27.10]
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 235.88it/s, loss=24, val_loss=32.40, train_loss=27.10]
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:00&lt;00:00, 235.39it/s, loss=25.1, val_loss=32.40, train_loss=27.10]
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 236.61it/s, loss=25.1, val_loss=32.40, train_loss=27.10]
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:00&lt;00:00, 236.14it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 236.99it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:00&lt;00:00, 236.62it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 237.56it/s, loss=25.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:00&lt;00:00, 237.08it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 237.53it/s, loss=26.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:00&lt;00:00, 237.22it/s, loss=27.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 238.54it/s, loss=27.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:00&lt;00:00, 238.10it/s, loss=28, val_loss=32.40, train_loss=27.10]
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 238.01it/s, loss=28, val_loss=32.40, train_loss=27.10]
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:00&lt;00:00, 237.71it/s, loss=28.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 239.07it/s, loss=28.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:00&lt;00:00, 238.68it/s, loss=29, val_loss=32.40, train_loss=27.10]
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 238.71it/s, loss=29, val_loss=32.40, train_loss=27.10]
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:00&lt;00:00, 238.35it/s, loss=27.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 238.86it/s, loss=27.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:00&lt;00:00, 238.56it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 239.55it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:00&lt;00:00, 239.25it/s, loss=28.5, val_loss=32.40, train_loss=27.10]
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 240.37it/s, loss=28.5, val_loss=32.40, train_loss=27.10]
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:00&lt;00:00, 240.13it/s, loss=28.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 241.37it/s, loss=28.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:00&lt;00:00, 241.03it/s, loss=29.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 241.02it/s, loss=29.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:00&lt;00:00, 240.72it/s, loss=29.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 241.34it/s, loss=29.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:00&lt;00:00, 241.09it/s, loss=29.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 242.07it/s, loss=29.3, val_loss=32.40, train_loss=27.10]
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:00&lt;00:00, 241.74it/s, loss=28.9, val_loss=32.40, train_loss=27.10]
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 241.40it/s, loss=28.9, val_loss=32.40, train_loss=27.10]
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:00&lt;00:00, 241.15it/s, loss=29.1, val_loss=32.40, train_loss=27.10]
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 241.94it/s, loss=29.1, val_loss=32.40, train_loss=27.10]
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:00&lt;00:00, 241.66it/s, loss=30.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 241.77it/s, loss=30.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:00&lt;00:00, 241.52it/s, loss=29.9, val_loss=32.40, train_loss=27.10]
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 242.23it/s, loss=29.9, val_loss=32.40, train_loss=27.10]
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:00&lt;00:00, 241.98it/s, loss=30.8, val_loss=32.40, train_loss=27.10]
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 242.58it/s, loss=30.8, val_loss=32.40, train_loss=27.10]
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:00&lt;00:00, 242.34it/s, loss=30.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 243.00it/s, loss=30.7, val_loss=32.40, train_loss=27.10]
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:00&lt;00:00, 242.74it/s, loss=30, val_loss=32.40, train_loss=27.10]
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 243.10it/s, loss=30, val_loss=32.40, train_loss=27.10]
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:00&lt;00:00, 242.91it/s, loss=28.8, val_loss=32.40, train_loss=27.10]
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 243.63it/s, loss=28.8, val_loss=32.40, train_loss=27.10]
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:00&lt;00:00, 243.38it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 243.54it/s, loss=27.6, val_loss=32.40, train_loss=27.10]
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:00&lt;00:00, 243.30it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 243.73it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:00&lt;00:00, 243.50it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 243.74it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:00&lt;00:00, 243.52it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 243.77it/s, loss=27.4, val_loss=32.40, train_loss=27.10]
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:00&lt;00:00, 243.56it/s, loss=27.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 243.93it/s, loss=27.2, val_loss=32.40, train_loss=27.10]
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:00&lt;00:00, 243.72it/s, loss=26.5, val_loss=32.40, train_loss=27.10]
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 244.04it/s, loss=26.5, val_loss=32.40, train_loss=27.10]
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:00&lt;00:00, 243.83it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:00&lt;00:00, 246.32it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:00&lt;00:00, 250.31it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:00&lt;00:00, 254.34it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:00&lt;00:00, 258.37it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:00&lt;00:00, 262.39it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:00&lt;00:00, 266.40it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:00&lt;00:00, 270.35it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:00&lt;00:00, 274.32it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:00&lt;00:00, 278.28it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:00&lt;00:00, 282.08it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:00&lt;00:00, 286.02it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:00&lt;00:00, 289.94it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 293.88it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 292.58it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 292.05it/s, loss=26, val_loss=32.40, train_loss=27.10]
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:00&lt;00:00, 285.05it/s, loss=26, val_loss=32.40, train_loss=27.10]
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     Validate metric           DataLoader 0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
         MAE_val             4.60679817199707
         R2_val           -0.0008908510208129883
        val_loss             32.36520004272461
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
</pre></div>
</div>
</section>
<section id="plotting-the-results-of-the-second-model">
<h2>7. Plotting the results of the second model<a class="headerlink" href="#plotting-the-results-of-the-second-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plotter</span> <span class="o">=</span> <span class="n">Plotter</span><span class="p">(</span><span class="n">single_model_dict</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">single_model_figures_dict</span> <span class="o">=</span> <span class="n">plotter</span><span class="o">.</span><span class="n">plot_all</span><span class="p">()</span>
<span class="n">plotter</span><span class="o">.</span><span class="n">show_all</span><span class="p">(</span><span class="n">single_model_figures_dict</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_two_models_traintest_002.png" srcset="../_images/sphx_glr_plot_two_models_traintest_002.png" alt="TabularChannelWiseMultiAttention - Validation R2: -0.001" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Plotting models [&#39;TabularChannelWiseMultiAttention&#39;] ...
Plotting results of a single model.
TabularChannelWiseMultiAttention_reals_vs_preds
</pre></div>
</div>
</section>
<section id="comparing-the-results-of-the-two-models">
<h2>8. Comparing the results of the two models<a class="headerlink" href="#comparing-the-results-of-the-two-models" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Now we‚Äôre going to compare the results of the two models. We‚Äôre using the same steps as when we used Plotter before, but this time we‚Äôre using the <code class="docutils literal notranslate"><span class="pre">all_trained_models</span></code> dictionary which contains both models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">comparison_plotter</span> <span class="o">=</span> <span class="n">Plotter</span><span class="p">(</span><span class="n">all_trained_models</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">comparison_plot_dict</span> <span class="o">=</span> <span class="n">comparison_plotter</span><span class="o">.</span><span class="n">plot_all</span><span class="p">()</span>
<span class="n">comparison_plotter</span><span class="o">.</span><span class="n">show_all</span><span class="p">(</span><span class="n">comparison_plot_dict</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_two_models_traintest_003.png" srcset="../_images/sphx_glr_plot_two_models_traintest_003.png" alt="Model Performance Comparison, R2, MAE" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Plotting models [&#39;ConcatTabularData&#39;, &#39;TabularChannelWiseMultiAttention&#39;] ...
Plotting comparison plots for 2 models: [&#39;ConcatTabularData&#39;, &#39;TabularChannelWiseMultiAttention&#39;]
compare_tt_models
</pre></div>
</div>
</section>
<section id="saving-the-metrics-of-the-two-models">
<h2>9. Saving the metrics of the two models<a class="headerlink" href="#saving-the-metrics-of-the-two-models" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>We can also get the metrics of the two models into a Pandas DataFrame using the <a class="reference internal" href="../autosummary/fusionlibrary.eval_functions.Plotter.html#fusionlibrary.eval_functions.Plotter.get_performance_df" title="fusionlibrary.eval_functions.Plotter.get_performance_df"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_performance_df()</span></code></a> function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">performances_df</span> <span class="o">=</span> <span class="n">comparison_plotter</span><span class="o">.</span><span class="n">get_performance_df</span><span class="p">()</span>
<span class="n">performances_df</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>R2</th>
      <th>MAE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ConcatTabularData</th>
      <td>0.037334</td>
      <td>4.126473</td>
    </tr>
    <tr>
      <th>TabularChannelWiseMultiAttention</th>
      <td>-0.000891</td>
      <td>4.606798</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 1.909 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-two-models-traintest-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/f3490f76b5e573ac3982ffa4cac13f17/plot_two_models_traintest.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_two_models_traintest.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3141f7895d8167efd5f524f2ba216b1c/plot_two_models_traintest.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_two_models_traintest.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_one_model_binary_kfold.html" class="btn btn-neutral float-left" title="Binary: training one kfold model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Creating%20your%20own%20fusion%20model/index.html" class="btn btn-neutral float-right" title="How to create your own fusion model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Florence J Townend.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>