
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_two_models_traintest.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_two_models_traintest.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_two_models_traintest.py:


Regression: comparing two tabular models trained on simulated data
====================================================================

This script shows how to train two fusion models on a regression task with train/test protocol and multimodal tabular data.

Key Features:

- Importing models based on name.
- Training and testing models with train/test protocol.
- Saving trained models to a dictionary for later analysis.
- Plotting the results of a single model.
- Plotting the results of multiple models as a bar chart.
- Saving the results of multiple models as a csv file.

.. GENERATED FROM PYTHON SOURCE LINES 16-30

.. code-block:: default


    import importlib

    import matplotlib.pyplot as plt
    from tqdm.auto import tqdm

    from docs.examples import generate_sklearn_simulated_data
    from fusionlibrary.datamodules import get_data_module
    from fusionlibrary.eval_functions import Plotter
    from fusionlibrary.fusion_models.base_pl_model import BaseModel
    from fusionlibrary.train_functions import train_and_save_models
    from fusionlibrary.utils.model_chooser import get_models









.. GENERATED FROM PYTHON SOURCE LINES 31-42

1. Import fusion models
------------------------
Here we import the fusion models to be compared. The models are imported using the
:func:`~fusionlibrary.utils.model_chooser.get_models` function, which takes a dictionary of conditions
as an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.

The function returns a dataframe of the models that match the conditions. The dataframe contains the
method name, the class name, the modality type, the fusion type, the path to the model, and the path to the
model's parent class. The paths are used to import the models with the :func:`importlib.import_module`.

We're importing ConcatTabularData and TabularChannelWiseMultiAttention models for this example. Both are multimodal tabular models.

.. GENERATED FROM PYTHON SOURCE LINES 42-59

.. code-block:: default


    model_conditions = {
        "class_name": ["ConcatTabularData", "TabularChannelWiseMultiAttention"],
    }

    imported_models = get_models(model_conditions)
    print("Imported methods:")
    print(imported_models.method_name.values)

    fusion_models = []  # contains the class objects for each model
    for index, row in imported_models.iterrows():
        module = importlib.import_module(row["method_path"])
        module_class = getattr(module, row["class_name"])

        fusion_models.append(module_class)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Imported methods:
    ['Concatenating tabular data' 'Channel-wise multiplication net (tabular)']




.. GENERATED FROM PYTHON SOURCE LINES 60-72

2. Set the training parameters
--------------------------------
Here we define the parameters for training and testing the models. The parameters are stored in a dictionary and passed to most
of the methods in this library.
For training and testing, the necessary parameters are:

- ``test_size``: the proportion of the data to be used for testing.
- ``kfold_flag``: the user sets this to False for train/test protocol.
- ``log``: a boolean of whether to log the results using Weights and Biases.
- ``pred_type``: the type of prediction to be performed. This is either ``regression``, ``binary``, or ``classification``. For this example we're using regression.

If we were going to use a subspace-based fusion model, we would also need to set the latent dimensionality of the subspace with ``subspace_latdims``. This will be shown in a different example.

.. GENERATED FROM PYTHON SOURCE LINES 72-81

.. code-block:: default


    params = {
        "test_size": 0.2,
        "kfold_flag": False,
        "log": False,
        "pred_type": "regression",
    }









.. GENERATED FROM PYTHON SOURCE LINES 82-86

3. Generating simulated data
--------------------------------
Here we generate simulated data for the two tabular modalities for this example.
This function also simulated image data which we aren't using here.

.. GENERATED FROM PYTHON SOURCE LINES 86-95

.. code-block:: default


    params = generate_sklearn_simulated_data(
        num_samples=500,
        num_tab1_features=10,
        num_tab2_features=10,
        img_dims=(1, 100, 100),
        params=params,
    )








.. GENERATED FROM PYTHON SOURCE LINES 96-108

4. Training the first fusion model
----------------------------------
Here we train the first fusion model. We're using the ``train_and_save_models`` function to train and test the models.
This function takes the following inputs:

- ``trained_models_dict``: a dictionary to store the trained models.
- ``data_module``: the data module containing the data.
- ``params``: the parameters for training and testing.
- ``fusion_model``: the fusion model to be trained.
- ``init_model``: the initialised dummy fusion model.

First we'll create a dictionary to store both the trained models so we can compare them later.

.. GENERATED FROM PYTHON SOURCE LINES 108-110

.. code-block:: default

    all_trained_models = {}  # create dictionary to store trained models








.. GENERATED FROM PYTHON SOURCE LINES 111-120

To train the first model we need to:

1. *Choose the model*: We're using the first model in the ``fusion_models`` list we made earlier.
2. *Create a dictionary to store the trained model*: We're using the name of the model as the key. It may seem overkill to make a dictionary just to store one model, but we also use this when we do k-fold training to store the trained models from the different folds.
3. *Initialise the model with dummy data*: This is so we can find out whether there are extra instructions for creating the datamodule (such as a method for creating a graph datamodule).
4. *Print the attributes of the model*: To check it's been initialised correctly.
5. *Create the datamodule*: This is done with the :func:`~fusionlibrary.datamodules.get_data_module` function. This function takes the initialised model and the parameters as inputs. It returns the datamodule.
6. *Train and test the model*: This is done with the :func:`~fusionlibrary.train_functions.train_and_save_models` function. This function takes the trained_models_dict, the datamodule, the parameters, the fusion model, and the initialised model as inputs. It returns the trained_models_dict with the trained model added to it.
7. *Add the trained model to the ``all_trained_models`` dictionary*: This is so we can compare the results of the two models later.

.. GENERATED FROM PYTHON SOURCE LINES 120-152

.. code-block:: default


    fusion_model = fusion_models[0]
    single_model_dict = {}

    # Initialise model
    init_model = BaseModel(
        fusion_model(
            params["pred_type"], data_dims=[10, 10, [100, 100, 100]], params=params
        )
    )

    print("Method name:", init_model.method_name)
    print("Modality type:", init_model.modality_type)
    print("Fusion type:", init_model.fusion_type)
    print("Metrics:", init_model.metric_names_list)

    # Create the data module
    dm = get_data_module(init_model=init_model, params=params)

    # Train and test
    single_model_dict = train_and_save_models(
        trained_models_dict=single_model_dict,
        data_module=dm,
        params=params,
        fusion_model=fusion_model,
        init_model=init_model,
    )

    # Add trained model to dictionary
    all_trained_models[fusion_model.__name__] = single_model_dict[fusion_model.__name__]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Method name: Concatenating tabular data
    Modality type: both_tab
    Fusion type: operation
    Metrics: ['R2', 'MAE']
    Training: 0it [00:00, ?it/s]    Training:   0%|          | 0/63 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s]     Epoch 0:   2%|▏         | 1/63 [00:00<00:02, 21.79it/s]    Epoch 0:   2%|▏         | 1/63 [00:00<00:02, 21.41it/s, loss=65.7]    Epoch 0:   3%|▎         | 2/63 [00:00<00:01, 40.42it/s, loss=65.7]    Epoch 0:   3%|▎         | 2/63 [00:00<00:01, 40.27it/s, loss=47.8]    Epoch 0:   5%|▍         | 3/63 [00:00<00:01, 58.03it/s, loss=47.8]    Epoch 0:   5%|▍         | 3/63 [00:00<00:01, 57.88it/s, loss=42]      Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 74.66it/s, loss=42]    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 74.37it/s, loss=41]    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 89.29it/s, loss=41]    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 89.04it/s, loss=37.5]    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 102.89it/s, loss=37.5]    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 102.57it/s, loss=35.1]    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 115.72it/s, loss=35.1]    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 115.41it/s, loss=33.1]    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 127.49it/s, loss=33.1]    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 127.15it/s, loss=32.1]    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 138.73it/s, loss=32.1]    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 138.40it/s, loss=32.7]    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 149.54it/s, loss=32.7]    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 149.22it/s, loss=30.8]    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 159.37it/s, loss=30.8]    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 159.08it/s, loss=29.4]    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 169.17it/s, loss=29.4]    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 168.79it/s, loss=29.1]    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 178.39it/s, loss=29.1]    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 177.95it/s, loss=29.6]    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 186.70it/s, loss=29.6]    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 186.36it/s, loss=28.9]    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 195.20it/s, loss=28.9]    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 194.71it/s, loss=28.6]    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 202.34it/s, loss=28.6]    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 201.95it/s, loss=29.8]    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 210.04it/s, loss=29.8]    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 209.74it/s, loss=29.4]    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 217.41it/s, loss=29.4]    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 216.92it/s, loss=30.3]    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 223.19it/s, loss=30.3]    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 222.79it/s, loss=30.7]    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 229.91it/s, loss=30.7]    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 229.58it/s, loss=30.2]    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 236.31it/s, loss=30.2]    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 235.90it/s, loss=29.1]    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 242.23it/s, loss=29.1]    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 241.93it/s, loss=29.7]    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 248.20it/s, loss=29.7]    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 247.71it/s, loss=29]      Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 252.75it/s, loss=29]    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 252.36it/s, loss=29.6]    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 258.08it/s, loss=29.6]    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 257.68it/s, loss=29.7]    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 263.29it/s, loss=29.7]    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 262.93it/s, loss=31.8]    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 268.39it/s, loss=31.8]    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 267.95it/s, loss=32.1]    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 272.27it/s, loss=32.1]    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 271.92it/s, loss=33.4]    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 276.79it/s, loss=33.4]    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 276.39it/s, loss=33.8]    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 281.20it/s, loss=33.8]    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 280.85it/s, loss=33.9]    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 285.89it/s, loss=33.9]    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 285.51it/s, loss=34.3]    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 290.15it/s, loss=34.3]    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 289.81it/s, loss=33.9]    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 294.29it/s, loss=33.9]    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 293.82it/s, loss=32.4]    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 296.18it/s, loss=32.4]    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 295.81it/s, loss=33.4]    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 299.89it/s, loss=33.4]    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 299.55it/s, loss=32.7]    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 303.80it/s, loss=32.7]    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 303.36it/s, loss=32.9]    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 306.60it/s, loss=32.9]    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 306.06it/s, loss=32]      Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 309.59it/s, loss=32]    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 309.21it/s, loss=30.9]    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 312.80it/s, loss=30.9]    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 312.34it/s, loss=29.5]    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 315.54it/s, loss=29.5]    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 315.19it/s, loss=30.2]    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 318.48it/s, loss=30.2]    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 317.93it/s, loss=29.1]    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 321.49it/s, loss=29.1]    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 321.22it/s, loss=28]      Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 324.90it/s, loss=28]    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 324.47it/s, loss=28.2]    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 326.96it/s, loss=28.2]    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 326.44it/s, loss=27.4]    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 329.57it/s, loss=27.4]    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 329.19it/s, loss=27.2]    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 331.88it/s, loss=27.2]    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 331.54it/s, loss=24.7]    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 334.48it/s, loss=24.7]    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 334.07it/s, loss=23.5]    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 336.79it/s, loss=23.5]    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 336.47it/s, loss=21.6]    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 339.37it/s, loss=21.6]    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 339.03it/s, loss=20.9]    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 341.71it/s, loss=20.9]    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 341.35it/s, loss=21.2]    Epoch 0:  81%|████████  | 51/63 [00:00<00:00, 344.15it/s, loss=21.2]    Epoch 0:  83%|████████▎ | 52/63 [00:00<00:00, 349.60it/s, loss=21.2]    Epoch 0:  84%|████████▍ | 53/63 [00:00<00:00, 355.26it/s, loss=21.2]    Epoch 0:  86%|████████▌ | 54/63 [00:00<00:00, 360.88it/s, loss=21.2]    Epoch 0:  87%|████████▋ | 55/63 [00:00<00:00, 366.51it/s, loss=21.2]    Epoch 0:  89%|████████▉ | 56/63 [00:00<00:00, 372.14it/s, loss=21.2]    Epoch 0:  90%|█████████ | 57/63 [00:00<00:00, 377.69it/s, loss=21.2]    Epoch 0:  92%|█████████▏| 58/63 [00:00<00:00, 383.19it/s, loss=21.2]    Epoch 0:  94%|█████████▎| 59/63 [00:00<00:00, 388.68it/s, loss=21.2]    Epoch 0:  95%|█████████▌| 60/63 [00:00<00:00, 394.12it/s, loss=21.2]    Epoch 0:  97%|█████████▋| 61/63 [00:00<00:00, 399.54it/s, loss=21.2]    Epoch 0:  98%|█████████▊| 62/63 [00:00<00:00, 405.00it/s, loss=21.2]    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 410.47it/s, loss=21.2]    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 406.07it/s, loss=21.2, val_loss=28.00]    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 401.32it/s, loss=21.2, val_loss=28.00, train_loss=28.20]    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s, loss=21.2, val_loss=28.00, train_loss=28.20]              Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=21.2, val_loss=28.00, train_loss=28.20]    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 446.82it/s, loss=21.2, val_loss=28.00, train_loss=28.20]    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 416.47it/s, loss=20.9, val_loss=28.00, train_loss=28.20]    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 452.80it/s, loss=20.9, val_loss=28.00, train_loss=28.20]    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 433.18it/s, loss=21.1, val_loss=28.00, train_loss=28.20]    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 462.45it/s, loss=21.1, val_loss=28.00, train_loss=28.20]    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 451.21it/s, loss=21.7, val_loss=28.00, train_loss=28.20]    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 463.83it/s, loss=21.7, val_loss=28.00, train_loss=28.20]    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 455.89it/s, loss=21.8, val_loss=28.00, train_loss=28.20]    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 479.12it/s, loss=21.8, val_loss=28.00, train_loss=28.20]    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 471.51it/s, loss=23, val_loss=28.00, train_loss=28.20]      Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 474.16it/s, loss=23, val_loss=28.00, train_loss=28.20]    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 468.24it/s, loss=21.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 480.28it/s, loss=21.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 475.71it/s, loss=24.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 486.86it/s, loss=24.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 481.99it/s, loss=24.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 483.51it/s, loss=24.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 480.13it/s, loss=25.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 493.34it/s, loss=25.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 490.49it/s, loss=24.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 498.69it/s, loss=24.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 494.30it/s, loss=25.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 494.42it/s, loss=25.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 491.64it/s, loss=26.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 499.86it/s, loss=26.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 497.26it/s, loss=26.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 503.58it/s, loss=26.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 501.26it/s, loss=25.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 500.43it/s, loss=25.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 497.71it/s, loss=26, val_loss=28.00, train_loss=28.20]      Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 494.79it/s, loss=26, val_loss=28.00, train_loss=28.20]    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 492.31it/s, loss=26.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 495.18it/s, loss=26.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 492.98it/s, loss=27.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 497.32it/s, loss=27.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 495.36it/s, loss=28.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 497.51it/s, loss=28.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 495.33it/s, loss=29.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 490.68it/s, loss=29.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 488.78it/s, loss=30.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 492.04it/s, loss=30.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 490.58it/s, loss=31.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 494.42it/s, loss=31.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 492.32it/s, loss=31.4, val_loss=28.00, train_loss=28.20]    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 491.55it/s, loss=31.4, val_loss=28.00, train_loss=28.20]    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 489.98it/s, loss=31.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 492.58it/s, loss=31.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 491.24it/s, loss=32, val_loss=28.00, train_loss=28.20]      Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 494.29it/s, loss=32, val_loss=28.00, train_loss=28.20]    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 493.09it/s, loss=31.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 497.88it/s, loss=31.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 496.72it/s, loss=32.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 501.39it/s, loss=32.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 500.33it/s, loss=30.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 502.76it/s, loss=30.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 500.75it/s, loss=29.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 502.54it/s, loss=29.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 501.00it/s, loss=29.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 500.28it/s, loss=29.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 499.10it/s, loss=29.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 500.83it/s, loss=29.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 499.68it/s, loss=28.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 503.07it/s, loss=28.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 502.08it/s, loss=28.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 504.97it/s, loss=28.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 503.59it/s, loss=28.4, val_loss=28.00, train_loss=28.20]    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 503.35it/s, loss=28.4, val_loss=28.00, train_loss=28.20]    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 502.02it/s, loss=28.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 503.00it/s, loss=28.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 501.98it/s, loss=28.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 502.86it/s, loss=28.3, val_loss=28.00, train_loss=28.20]    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 501.89it/s, loss=28, val_loss=28.00, train_loss=28.20]      Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 501.95it/s, loss=28, val_loss=28.00, train_loss=28.20]    Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 500.75it/s, loss=27.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 501.67it/s, loss=27.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 500.65it/s, loss=27, val_loss=28.00, train_loss=28.20]      Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 502.45it/s, loss=27, val_loss=28.00, train_loss=28.20]    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 501.52it/s, loss=26.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 502.67it/s, loss=26.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 501.74it/s, loss=25.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 503.31it/s, loss=25.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 502.41it/s, loss=26.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 504.16it/s, loss=26.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 503.18it/s, loss=26.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 503.74it/s, loss=26.9, val_loss=28.00, train_loss=28.20]    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 502.70it/s, loss=26.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 503.15it/s, loss=26.6, val_loss=28.00, train_loss=28.20]    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 502.36it/s, loss=25.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 503.83it/s, loss=25.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 502.79it/s, loss=25.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 502.82it/s, loss=25.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 501.94it/s, loss=25.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 503.20it/s, loss=25.5, val_loss=28.00, train_loss=28.20]    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 502.47it/s, loss=24.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 504.04it/s, loss=24.7, val_loss=28.00, train_loss=28.20]    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 503.05it/s, loss=25.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 503.14it/s, loss=25.2, val_loss=28.00, train_loss=28.20]    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 502.43it/s, loss=24.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 504.69it/s, loss=24.8, val_loss=28.00, train_loss=28.20]    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 504.11it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  81%|████████  | 51/63 [00:00<00:00, 506.98it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  83%|████████▎ | 52/63 [00:00<00:00, 514.67it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  84%|████████▍ | 53/63 [00:00<00:00, 522.39it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  86%|████████▌ | 54/63 [00:00<00:00, 530.06it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  87%|████████▋ | 55/63 [00:00<00:00, 537.71it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  89%|████████▉ | 56/63 [00:00<00:00, 545.32it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  90%|█████████ | 57/63 [00:00<00:00, 552.87it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  92%|█████████▏| 58/63 [00:00<00:00, 560.37it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  94%|█████████▎| 59/63 [00:00<00:00, 567.78it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  95%|█████████▌| 60/63 [00:00<00:00, 575.19it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  97%|█████████▋| 61/63 [00:00<00:00, 582.54it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1:  98%|█████████▊| 62/63 [00:00<00:00, 589.85it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 597.17it/s, loss=26.1, val_loss=28.00, train_loss=28.20]    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 592.71it/s, loss=26.1, val_loss=27.80, train_loss=28.20]    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 590.52it/s, loss=26.1, val_loss=27.80, train_loss=28.00]    Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=26.1, val_loss=27.80, train_loss=28.00]              Epoch 2:   0%|          | 0/63 [00:00<?, ?it/s, loss=26.1, val_loss=27.80, train_loss=28.00]    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 447.44it/s, loss=26.1, val_loss=27.80, train_loss=28.00]    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 420.48it/s, loss=27.2, val_loss=27.80, train_loss=28.00]    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 487.45it/s, loss=27.2, val_loss=27.80, train_loss=28.00]    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 467.83it/s, loss=26.8, val_loss=27.80, train_loss=28.00]    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 481.55it/s, loss=26.8, val_loss=27.80, train_loss=28.00]    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 470.97it/s, loss=26.6, val_loss=27.80, train_loss=28.00]    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 488.05it/s, loss=26.6, val_loss=27.80, train_loss=28.00]    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 479.21it/s, loss=26.5, val_loss=27.80, train_loss=28.00]    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 488.81it/s, loss=26.5, val_loss=27.80, train_loss=28.00]    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 480.85it/s, loss=26.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 489.27it/s, loss=26.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 483.83it/s, loss=25.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 498.65it/s, loss=25.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 493.07it/s, loss=26.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 495.02it/s, loss=26.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 490.40it/s, loss=26.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 497.05it/s, loss=26.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 493.20it/s, loss=26.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 500.53it/s, loss=26.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 496.83it/s, loss=26.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 505.81it/s, loss=26.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 503.13it/s, loss=25.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 514.14it/s, loss=25.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 511.68it/s, loss=26.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 518.92it/s, loss=26.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 516.57it/s, loss=26.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 523.44it/s, loss=26.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 519.63it/s, loss=25.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 516.94it/s, loss=25.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 513.93it/s, loss=27.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 516.18it/s, loss=27.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 513.66it/s, loss=27.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 517.52it/s, loss=27.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 515.35it/s, loss=29.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 518.27it/s, loss=29.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 516.07it/s, loss=29.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 519.87it/s, loss=29.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 518.16it/s, loss=30.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 523.04it/s, loss=30.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 521.05it/s, loss=30.5, val_loss=27.80, train_loss=28.00]    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 521.45it/s, loss=30.5, val_loss=27.80, train_loss=28.00]    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 519.31it/s, loss=29.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 519.93it/s, loss=29.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 518.05it/s, loss=28.5, val_loss=27.80, train_loss=28.00]    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 520.51it/s, loss=28.5, val_loss=27.80, train_loss=28.00]    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 519.13it/s, loss=28.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 524.00it/s, loss=28.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 522.64it/s, loss=29.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 524.40it/s, loss=29.3, val_loss=27.80, train_loss=28.00]    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 522.41it/s, loss=30.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 522.16it/s, loss=30.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 520.77it/s, loss=30.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 523.46it/s, loss=30.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 521.79it/s, loss=30.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 523.20it/s, loss=30.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 521.78it/s, loss=31.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 523.17it/s, loss=31.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 521.72it/s, loss=32.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 522.87it/s, loss=32.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 521.31it/s, loss=31.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 522.10it/s, loss=31.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 520.90it/s, loss=29.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 522.71it/s, loss=29.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 521.50it/s, loss=29.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 523.52it/s, loss=29.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 522.32it/s, loss=29.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 524.03it/s, loss=29.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 523.02it/s, loss=29.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 524.00it/s, loss=29.2, val_loss=27.80, train_loss=28.00]    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 522.71it/s, loss=28.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 523.57it/s, loss=28.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 522.47it/s, loss=27.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 524.18it/s, loss=27.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 523.20it/s, loss=26.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 525.81it/s, loss=26.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 524.89it/s, loss=26.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 525.10it/s, loss=26.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 524.02it/s, loss=25.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 525.65it/s, loss=25.8, val_loss=27.80, train_loss=28.00]    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 524.82it/s, loss=24.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 526.61it/s, loss=24.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 525.32it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 524.18it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 523.25it/s, loss=25, val_loss=27.80, train_loss=28.00]      Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 524.03it/s, loss=25, val_loss=27.80, train_loss=28.00]    Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 523.19it/s, loss=24.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 524.03it/s, loss=24.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 523.09it/s, loss=24.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 523.35it/s, loss=24.9, val_loss=27.80, train_loss=28.00]    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 522.45it/s, loss=23.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 524.29it/s, loss=23.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 523.61it/s, loss=25.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 524.55it/s, loss=25.4, val_loss=27.80, train_loss=28.00]    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 523.63it/s, loss=24.7, val_loss=27.80, train_loss=28.00]    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 524.31it/s, loss=24.7, val_loss=27.80, train_loss=28.00]    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 523.56it/s, loss=24.7, val_loss=27.80, train_loss=28.00]    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 524.71it/s, loss=24.7, val_loss=27.80, train_loss=28.00]    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 524.00it/s, loss=24.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 525.43it/s, loss=24.6, val_loss=27.80, train_loss=28.00]    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 524.69it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  81%|████████  | 51/63 [00:00<00:00, 526.95it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  83%|████████▎ | 52/63 [00:00<00:00, 534.83it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  84%|████████▍ | 53/63 [00:00<00:00, 542.60it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  86%|████████▌ | 54/63 [00:00<00:00, 550.15it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  87%|████████▋ | 55/63 [00:00<00:00, 557.77it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  89%|████████▉ | 56/63 [00:00<00:00, 565.34it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  90%|█████████ | 57/63 [00:00<00:00, 572.76it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  92%|█████████▏| 58/63 [00:00<00:00, 580.15it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  94%|█████████▎| 59/63 [00:00<00:00, 587.47it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  95%|█████████▌| 60/63 [00:00<00:00, 594.73it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  97%|█████████▋| 61/63 [00:00<00:00, 599.44it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2:  98%|█████████▊| 62/63 [00:00<00:00, 605.31it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 612.07it/s, loss=25.1, val_loss=27.80, train_loss=28.00]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 606.53it/s, loss=25.1, val_loss=26.80, train_loss=28.00]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 602.98it/s, loss=25.1, val_loss=26.80, train_loss=27.50]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 582.33it/s, loss=25.1, val_loss=26.80, train_loss=27.50]
    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         Validate metric           DataLoader 0
    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
             MAE_val             4.126473426818848
             R2_val             0.03733396530151367
            val_loss            26.750049591064453
    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────




.. GENERATED FROM PYTHON SOURCE LINES 153-157

5. Plotting the results of the first model
--------------------------------------------
We're using the :class:`~fusionlibrary.eval_functions.Plotter` class to plot the results of the first model. This class takes the dictionary of trained models and the parameters as inputs. It returns a dictionary of figures.
If there is one model in the dictionary (i.e. only one unique key), then it plots the figures for analysing the results of a single model.

.. GENERATED FROM PYTHON SOURCE LINES 157-162

.. code-block:: default


    plotter = Plotter(single_model_dict, params)
    single_model_figures_dict = plotter.plot_all()
    plotter.show_all(single_model_figures_dict)




.. image-sg:: /auto_examples/images/sphx_glr_plot_two_models_traintest_001.png
   :alt: ConcatTabularData - Validation R2: 0.037
   :srcset: /auto_examples/images/sphx_glr_plot_two_models_traintest_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Plotting models ['ConcatTabularData'] ...
    Plotting results of a single model.
    ConcatTabularData_reals_vs_preds




.. GENERATED FROM PYTHON SOURCE LINES 163-166

6. Training the second fusion model
-------------------------------------
Here we train the second fusion model: TabularChannelWiseMultiAttention. We're using the same steps as before, but this time we're using the second model in the ``fusion_models`` list.

.. GENERATED FROM PYTHON SOURCE LINES 169-170

Choose the model

.. GENERATED FROM PYTHON SOURCE LINES 170-201

.. code-block:: default

    fusion_model = fusion_models[1]
    single_model_dict = {}

    # Initialise model
    init_model = BaseModel(
        fusion_model(
            params["pred_type"], data_dims=[10, 10, [100, 100, 100]], params=params
        )
    )

    print("Method name:", init_model.method_name)
    print("Modality type:", init_model.modality_type)
    print("Fusion type:", init_model.fusion_type)
    print("Metrics:", init_model.metric_names_list)

    # Create the data module
    dm = get_data_module(init_model=init_model, params=params)

    # Train and test
    trained_models = train_and_save_models(
        trained_models_dict=single_model_dict,
        data_module=dm,
        params=params,
        fusion_model=fusion_model,
        init_model=init_model,
    )

    # Add trained model to dictionary
    all_trained_models[fusion_model.__name__] = single_model_dict[fusion_model.__name__]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Method name: Channel-wise multiplication net (tabular)
    Modality type: both_tab
    Fusion type: attention
    Metrics: ['R2', 'MAE']
    Training: 0it [00:00, ?it/s]    Training:   0%|          | 0/63 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s]     Epoch 0:   2%|▏         | 1/63 [00:00<00:00, 95.62it/s]    Epoch 0:   2%|▏         | 1/63 [00:00<00:00, 93.66it/s, loss=27.4]    Epoch 0:   3%|▎         | 2/63 [00:00<00:00, 128.60it/s, loss=27.4]    Epoch 0:   3%|▎         | 2/63 [00:00<00:00, 126.89it/s, loss=21.2]    Epoch 0:   5%|▍         | 3/63 [00:00<00:00, 142.71it/s, loss=21.2]    Epoch 0:   5%|▍         | 3/63 [00:00<00:00, 141.03it/s, loss=27.3]    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 153.58it/s, loss=27.3]    Epoch 0:   6%|▋         | 4/63 [00:00<00:00, 152.50it/s, loss=30.4]    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 161.44it/s, loss=30.4]    Epoch 0:   8%|▊         | 5/63 [00:00<00:00, 160.46it/s, loss=33.7]    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 167.50it/s, loss=33.7]    Epoch 0:  10%|▉         | 6/63 [00:00<00:00, 166.57it/s, loss=30.9]    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 172.78it/s, loss=30.9]    Epoch 0:  11%|█         | 7/63 [00:00<00:00, 171.98it/s, loss=31]      Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 178.25it/s, loss=31]    Epoch 0:  13%|█▎        | 8/63 [00:00<00:00, 177.45it/s, loss=32.2]    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 180.89it/s, loss=32.2]    Epoch 0:  14%|█▍        | 9/63 [00:00<00:00, 180.26it/s, loss=30.4]    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 183.99it/s, loss=30.4]    Epoch 0:  16%|█▌        | 10/63 [00:00<00:00, 183.37it/s, loss=30.7]    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 183.14it/s, loss=30.7]    Epoch 0:  17%|█▋        | 11/63 [00:00<00:00, 182.71it/s, loss=33.5]    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 186.88it/s, loss=33.5]    Epoch 0:  19%|█▉        | 12/63 [00:00<00:00, 186.33it/s, loss=33.6]    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 187.72it/s, loss=33.6]    Epoch 0:  21%|██        | 13/63 [00:00<00:00, 187.32it/s, loss=32.7]    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 190.30it/s, loss=32.7]    Epoch 0:  22%|██▏       | 14/63 [00:00<00:00, 189.82it/s, loss=31.6]    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 192.19it/s, loss=31.6]    Epoch 0:  24%|██▍       | 15/63 [00:00<00:00, 191.76it/s, loss=31.3]    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 193.48it/s, loss=31.3]    Epoch 0:  25%|██▌       | 16/63 [00:00<00:00, 193.08it/s, loss=30.7]    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 194.86it/s, loss=30.7]    Epoch 0:  27%|██▋       | 17/63 [00:00<00:00, 194.48it/s, loss=29.8]    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 195.88it/s, loss=29.8]    Epoch 0:  29%|██▊       | 18/63 [00:00<00:00, 195.52it/s, loss=29.1]    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 197.92it/s, loss=29.1]    Epoch 0:  30%|███       | 19/63 [00:00<00:00, 197.58it/s, loss=28.9]    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 200.15it/s, loss=28.9]    Epoch 0:  32%|███▏      | 20/63 [00:00<00:00, 199.80it/s, loss=29.5]    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 201.46it/s, loss=29.5]    Epoch 0:  33%|███▎      | 21/63 [00:00<00:00, 201.17it/s, loss=29.5]    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 203.90it/s, loss=29.5]    Epoch 0:  35%|███▍      | 22/63 [00:00<00:00, 203.67it/s, loss=30.3]    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 206.20it/s, loss=30.3]    Epoch 0:  37%|███▋      | 23/63 [00:00<00:00, 205.86it/s, loss=29.4]    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 207.07it/s, loss=29.4]    Epoch 0:  38%|███▊      | 24/63 [00:00<00:00, 206.79it/s, loss=27.9]    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 208.33it/s, loss=27.9]    Epoch 0:  40%|███▉      | 25/63 [00:00<00:00, 208.04it/s, loss=26.3]    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 209.84it/s, loss=26.3]    Epoch 0:  41%|████▏     | 26/63 [00:00<00:00, 209.62it/s, loss=26.5]    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 211.84it/s, loss=26.5]    Epoch 0:  43%|████▎     | 27/63 [00:00<00:00, 211.52it/s, loss=25.9]    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 212.65it/s, loss=25.9]    Epoch 0:  44%|████▍     | 28/63 [00:00<00:00, 212.40it/s, loss=24.4]    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 214.35it/s, loss=24.4]    Epoch 0:  46%|████▌     | 29/63 [00:00<00:00, 214.09it/s, loss=25.2]    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 214.98it/s, loss=25.2]    Epoch 0:  48%|████▊     | 30/63 [00:00<00:00, 214.71it/s, loss=24.8]    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 215.79it/s, loss=24.8]    Epoch 0:  49%|████▉     | 31/63 [00:00<00:00, 215.52it/s, loss=23.8]    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 216.60it/s, loss=23.8]    Epoch 0:  51%|█████     | 32/63 [00:00<00:00, 216.39it/s, loss=22.7]    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 218.07it/s, loss=22.7]    Epoch 0:  52%|█████▏    | 33/63 [00:00<00:00, 217.85it/s, loss=23.4]    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 219.50it/s, loss=23.4]    Epoch 0:  54%|█████▍    | 34/63 [00:00<00:00, 219.29it/s, loss=24]      Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 220.70it/s, loss=24]    Epoch 0:  56%|█████▌    | 35/63 [00:00<00:00, 220.52it/s, loss=23.2]    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 222.16it/s, loss=23.2]    Epoch 0:  57%|█████▋    | 36/63 [00:00<00:00, 221.91it/s, loss=23.6]    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 222.53it/s, loss=23.6]    Epoch 0:  59%|█████▊    | 37/63 [00:00<00:00, 222.35it/s, loss=23.8]    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 223.50it/s, loss=23.8]    Epoch 0:  60%|██████    | 38/63 [00:00<00:00, 223.26it/s, loss=24.5]    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 224.01it/s, loss=24.5]    Epoch 0:  62%|██████▏   | 39/63 [00:00<00:00, 223.84it/s, loss=25]      Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 224.88it/s, loss=25]    Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 224.66it/s, loss=24.9]    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 225.29it/s, loss=24.9]    Epoch 0:  65%|██████▌   | 41/63 [00:00<00:00, 225.09it/s, loss=25.3]    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 225.92it/s, loss=25.3]    Epoch 0:  67%|██████▋   | 42/63 [00:00<00:00, 225.72it/s, loss=24.4]    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 226.28it/s, loss=24.4]    Epoch 0:  68%|██████▊   | 43/63 [00:00<00:00, 226.08it/s, loss=24.7]    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 226.78it/s, loss=24.7]    Epoch 0:  70%|██████▉   | 44/63 [00:00<00:00, 226.53it/s, loss=26.1]    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 226.91it/s, loss=26.1]    Epoch 0:  71%|███████▏  | 45/63 [00:00<00:00, 226.72it/s, loss=26]      Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 227.33it/s, loss=26]    Epoch 0:  73%|███████▎  | 46/63 [00:00<00:00, 227.15it/s, loss=26.5]    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 227.60it/s, loss=26.5]    Epoch 0:  75%|███████▍  | 47/63 [00:00<00:00, 227.38it/s, loss=27.4]    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 227.70it/s, loss=27.4]    Epoch 0:  76%|███████▌  | 48/63 [00:00<00:00, 227.53it/s, loss=27.3]    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 228.27it/s, loss=27.3]    Epoch 0:  78%|███████▊  | 49/63 [00:00<00:00, 228.14it/s, loss=27.9]    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 229.17it/s, loss=27.9]    Epoch 0:  79%|███████▉  | 50/63 [00:00<00:00, 228.98it/s, loss=27.6]    Epoch 0:  81%|████████  | 51/63 [00:00<00:00, 231.68it/s, loss=27.6]    Epoch 0:  83%|████████▎ | 52/63 [00:00<00:00, 235.42it/s, loss=27.6]    Epoch 0:  84%|████████▍ | 53/63 [00:00<00:00, 239.24it/s, loss=27.6]    Epoch 0:  86%|████████▌ | 54/63 [00:00<00:00, 243.05it/s, loss=27.6]    Epoch 0:  87%|████████▋ | 55/63 [00:00<00:00, 246.84it/s, loss=27.6]    Epoch 0:  89%|████████▉ | 56/63 [00:00<00:00, 250.57it/s, loss=27.6]    Epoch 0:  90%|█████████ | 57/63 [00:00<00:00, 254.29it/s, loss=27.6]    Epoch 0:  92%|█████████▏| 58/63 [00:00<00:00, 258.00it/s, loss=27.6]    Epoch 0:  94%|█████████▎| 59/63 [00:00<00:00, 261.69it/s, loss=27.6]    Epoch 0:  95%|█████████▌| 60/63 [00:00<00:00, 265.32it/s, loss=27.6]    Epoch 0:  97%|█████████▋| 61/63 [00:00<00:00, 268.98it/s, loss=27.6]    Epoch 0:  98%|█████████▊| 62/63 [00:00<00:00, 272.57it/s, loss=27.6]    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 276.17it/s, loss=27.6]    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 273.86it/s, loss=27.6, val_loss=32.40]    Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 273.34it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s, loss=27.6, val_loss=32.40, train_loss=27.10]              Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 229.94it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:   2%|▏         | 1/63 [00:00<00:00, 220.31it/s, loss=26.4, val_loss=32.40, train_loss=27.10]    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 229.41it/s, loss=26.4, val_loss=32.40, train_loss=27.10]    Epoch 1:   3%|▎         | 2/63 [00:00<00:00, 224.19it/s, loss=27, val_loss=32.40, train_loss=27.10]      Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 228.01it/s, loss=27, val_loss=32.40, train_loss=27.10]    Epoch 1:   5%|▍         | 3/63 [00:00<00:00, 224.72it/s, loss=26.1, val_loss=32.40, train_loss=27.10]    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 231.68it/s, loss=26.1, val_loss=32.40, train_loss=27.10]    Epoch 1:   6%|▋         | 4/63 [00:00<00:00, 229.36it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 236.10it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 1:   8%|▊         | 5/63 [00:00<00:00, 234.21it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 239.71it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  10%|▉         | 6/63 [00:00<00:00, 238.11it/s, loss=27.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 239.30it/s, loss=27.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  11%|█         | 7/63 [00:00<00:00, 237.88it/s, loss=27, val_loss=32.40, train_loss=27.10]      Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 241.18it/s, loss=27, val_loss=32.40, train_loss=27.10]    Epoch 1:  13%|█▎        | 8/63 [00:00<00:00, 239.94it/s, loss=26.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 240.07it/s, loss=26.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  14%|█▍        | 9/63 [00:00<00:00, 238.96it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 238.77it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  16%|█▌        | 10/63 [00:00<00:00, 237.74it/s, loss=26.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 239.67it/s, loss=26.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  17%|█▋        | 11/63 [00:00<00:00, 238.84it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 240.44it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  19%|█▉        | 12/63 [00:00<00:00, 239.48it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 240.28it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  21%|██        | 13/63 [00:00<00:00, 239.50it/s, loss=25.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 240.58it/s, loss=25.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  22%|██▏       | 14/63 [00:00<00:00, 240.03it/s, loss=25.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 240.62it/s, loss=25.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  24%|██▍       | 15/63 [00:00<00:00, 239.80it/s, loss=25.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 239.18it/s, loss=25.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  25%|██▌       | 16/63 [00:00<00:00, 238.59it/s, loss=25.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 240.46it/s, loss=25.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  27%|██▋       | 17/63 [00:00<00:00, 239.94it/s, loss=24.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 241.54it/s, loss=24.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  29%|██▊       | 18/63 [00:00<00:00, 241.04it/s, loss=25.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 241.80it/s, loss=25.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  30%|███       | 19/63 [00:00<00:00, 241.28it/s, loss=25.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 242.32it/s, loss=25.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  32%|███▏      | 20/63 [00:00<00:00, 241.82it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 243.12it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  33%|███▎      | 21/63 [00:00<00:00, 242.69it/s, loss=27.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 242.42it/s, loss=27.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  35%|███▍      | 22/63 [00:00<00:00, 241.88it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 241.78it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  37%|███▋      | 23/63 [00:00<00:00, 241.34it/s, loss=28.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 242.09it/s, loss=28.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  38%|███▊      | 24/63 [00:00<00:00, 241.64it/s, loss=27.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 242.15it/s, loss=27.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  40%|███▉      | 25/63 [00:00<00:00, 241.76it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 242.16it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  41%|████▏     | 26/63 [00:00<00:00, 241.78it/s, loss=27.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 242.49it/s, loss=27.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  43%|████▎     | 27/63 [00:00<00:00, 242.15it/s, loss=28, val_loss=32.40, train_loss=27.10]      Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 243.18it/s, loss=28, val_loss=32.40, train_loss=27.10]    Epoch 1:  44%|████▍     | 28/63 [00:00<00:00, 242.90it/s, loss=27.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 243.61it/s, loss=27.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  46%|████▌     | 29/63 [00:00<00:00, 243.22it/s, loss=27.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 242.75it/s, loss=27.9, val_loss=32.40, train_loss=27.10]    Epoch 1:  48%|████▊     | 30/63 [00:00<00:00, 242.48it/s, loss=28.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 243.46it/s, loss=28.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  49%|████▉     | 31/63 [00:00<00:00, 243.20it/s, loss=28.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 243.89it/s, loss=28.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  51%|█████     | 32/63 [00:00<00:00, 243.47it/s, loss=30.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 243.90it/s, loss=30.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  52%|█████▏    | 33/63 [00:00<00:00, 243.65it/s, loss=29.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 244.35it/s, loss=29.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  54%|█████▍    | 34/63 [00:00<00:00, 244.12it/s, loss=28.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 244.65it/s, loss=28.7, val_loss=32.40, train_loss=27.10]    Epoch 1:  56%|█████▌    | 35/63 [00:00<00:00, 244.29it/s, loss=29.8, val_loss=32.40, train_loss=27.10]    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 244.61it/s, loss=29.8, val_loss=32.40, train_loss=27.10]    Epoch 1:  57%|█████▋    | 36/63 [00:00<00:00, 244.31it/s, loss=30, val_loss=32.40, train_loss=27.10]      Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 245.09it/s, loss=30, val_loss=32.40, train_loss=27.10]    Epoch 1:  59%|█████▊    | 37/63 [00:00<00:00, 244.78it/s, loss=30.4, val_loss=32.40, train_loss=27.10]    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 244.86it/s, loss=30.4, val_loss=32.40, train_loss=27.10]    Epoch 1:  60%|██████    | 38/63 [00:00<00:00, 244.55it/s, loss=29.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 245.01it/s, loss=29.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  62%|██████▏   | 39/63 [00:00<00:00, 244.78it/s, loss=28.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 245.21it/s, loss=28.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  63%|██████▎   | 40/63 [00:00<00:00, 244.93it/s, loss=26.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 245.41it/s, loss=26.1, val_loss=32.40, train_loss=27.10]    Epoch 1:  65%|██████▌   | 41/63 [00:00<00:00, 245.22it/s, loss=26.2, val_loss=32.40, train_loss=27.10]    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 245.36it/s, loss=26.2, val_loss=32.40, train_loss=27.10]    Epoch 1:  67%|██████▋   | 42/63 [00:00<00:00, 245.08it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 244.95it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 1:  68%|██████▊   | 43/63 [00:00<00:00, 244.70it/s, loss=26.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 245.11it/s, loss=26.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  70%|██████▉   | 44/63 [00:00<00:00, 244.84it/s, loss=26.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 244.89it/s, loss=26.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  71%|███████▏  | 45/63 [00:00<00:00, 244.69it/s, loss=27.8, val_loss=32.40, train_loss=27.10]    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 245.22it/s, loss=27.8, val_loss=32.40, train_loss=27.10]    Epoch 1:  73%|███████▎  | 46/63 [00:00<00:00, 245.06it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 245.81it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  75%|███████▍  | 47/63 [00:00<00:00, 245.56it/s, loss=27.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 245.60it/s, loss=27.5, val_loss=32.40, train_loss=27.10]    Epoch 1:  76%|███████▌  | 48/63 [00:00<00:00, 245.40it/s, loss=28.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 246.19it/s, loss=28.3, val_loss=32.40, train_loss=27.10]    Epoch 1:  78%|███████▊  | 49/63 [00:00<00:00, 245.99it/s, loss=28.4, val_loss=32.40, train_loss=27.10]    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 246.46it/s, loss=28.4, val_loss=32.40, train_loss=27.10]    Epoch 1:  79%|███████▉  | 50/63 [00:00<00:00, 246.30it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  81%|████████  | 51/63 [00:00<00:00, 249.20it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  83%|████████▎ | 52/63 [00:00<00:00, 253.33it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  84%|████████▍ | 53/63 [00:00<00:00, 257.49it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  86%|████████▌ | 54/63 [00:00<00:00, 261.60it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  87%|████████▋ | 55/63 [00:00<00:00, 265.71it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  89%|████████▉ | 56/63 [00:00<00:00, 269.79it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  90%|█████████ | 57/63 [00:00<00:00, 273.85it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  92%|█████████▏| 58/63 [00:00<00:00, 277.87it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  94%|█████████▎| 59/63 [00:00<00:00, 281.89it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  95%|█████████▌| 60/63 [00:00<00:00, 285.90it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  97%|█████████▋| 61/63 [00:00<00:00, 289.88it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:  98%|█████████▊| 62/63 [00:00<00:00, 293.84it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 297.81it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 296.63it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 295.83it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s, loss=26.6, val_loss=32.40, train_loss=27.10]              Epoch 2:   0%|          | 0/63 [00:00<?, ?it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 241.41it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 2:   2%|▏         | 1/63 [00:00<00:00, 231.42it/s, loss=25.9, val_loss=32.40, train_loss=27.10]    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 239.52it/s, loss=25.9, val_loss=32.40, train_loss=27.10]    Epoch 2:   3%|▎         | 2/63 [00:00<00:00, 234.55it/s, loss=24.7, val_loss=32.40, train_loss=27.10]    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 241.35it/s, loss=24.7, val_loss=32.40, train_loss=27.10]    Epoch 2:   5%|▍         | 3/63 [00:00<00:00, 238.46it/s, loss=25.9, val_loss=32.40, train_loss=27.10]    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 247.11it/s, loss=25.9, val_loss=32.40, train_loss=27.10]    Epoch 2:   6%|▋         | 4/63 [00:00<00:00, 244.32it/s, loss=26.2, val_loss=32.40, train_loss=27.10]    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 246.88it/s, loss=26.2, val_loss=32.40, train_loss=27.10]    Epoch 2:   8%|▊         | 5/63 [00:00<00:00, 244.82it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 243.71it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  10%|▉         | 6/63 [00:00<00:00, 241.95it/s, loss=25, val_loss=32.40, train_loss=27.10]      Epoch 2:  11%|█         | 7/63 [00:00<00:00, 245.55it/s, loss=25, val_loss=32.40, train_loss=27.10]    Epoch 2:  11%|█         | 7/63 [00:00<00:00, 243.74it/s, loss=24.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 248.02it/s, loss=24.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  13%|█▎        | 8/63 [00:00<00:00, 246.81it/s, loss=26.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 248.90it/s, loss=26.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  14%|█▍        | 9/63 [00:00<00:00, 247.80it/s, loss=27.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 237.91it/s, loss=27.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  16%|█▌        | 10/63 [00:00<00:00, 236.89it/s, loss=28.5, val_loss=32.40, train_loss=27.10]    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 237.61it/s, loss=28.5, val_loss=32.40, train_loss=27.10]    Epoch 2:  17%|█▋        | 11/63 [00:00<00:00, 236.71it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 239.23it/s, loss=27.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  19%|█▉        | 12/63 [00:00<00:00, 238.32it/s, loss=25.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 238.04it/s, loss=25.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  21%|██        | 13/63 [00:00<00:00, 237.11it/s, loss=25.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 235.12it/s, loss=25.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  22%|██▏       | 14/63 [00:00<00:00, 234.29it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 234.96it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  24%|██▍       | 15/63 [00:00<00:00, 234.22it/s, loss=24.8, val_loss=32.40, train_loss=27.10]    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 235.41it/s, loss=24.8, val_loss=32.40, train_loss=27.10]    Epoch 2:  25%|██▌       | 16/63 [00:00<00:00, 234.79it/s, loss=25.9, val_loss=32.40, train_loss=27.10]    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 235.56it/s, loss=25.9, val_loss=32.40, train_loss=27.10]    Epoch 2:  27%|██▋       | 17/63 [00:00<00:00, 234.92it/s, loss=25.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 235.19it/s, loss=25.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  29%|██▊       | 18/63 [00:00<00:00, 234.67it/s, loss=23.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 235.78it/s, loss=23.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  30%|███       | 19/63 [00:00<00:00, 235.29it/s, loss=23.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 236.07it/s, loss=23.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  32%|███▏      | 20/63 [00:00<00:00, 235.50it/s, loss=24, val_loss=32.40, train_loss=27.10]      Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 235.88it/s, loss=24, val_loss=32.40, train_loss=27.10]    Epoch 2:  33%|███▎      | 21/63 [00:00<00:00, 235.39it/s, loss=25.1, val_loss=32.40, train_loss=27.10]    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 236.61it/s, loss=25.1, val_loss=32.40, train_loss=27.10]    Epoch 2:  35%|███▍      | 22/63 [00:00<00:00, 236.14it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 236.99it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  37%|███▋      | 23/63 [00:00<00:00, 236.62it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 237.56it/s, loss=25.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  38%|███▊      | 24/63 [00:00<00:00, 237.08it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 237.53it/s, loss=26.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  40%|███▉      | 25/63 [00:00<00:00, 237.22it/s, loss=27.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 238.54it/s, loss=27.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  41%|████▏     | 26/63 [00:00<00:00, 238.10it/s, loss=28, val_loss=32.40, train_loss=27.10]      Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 238.01it/s, loss=28, val_loss=32.40, train_loss=27.10]    Epoch 2:  43%|████▎     | 27/63 [00:00<00:00, 237.71it/s, loss=28.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 239.07it/s, loss=28.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  44%|████▍     | 28/63 [00:00<00:00, 238.68it/s, loss=29, val_loss=32.40, train_loss=27.10]      Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 238.71it/s, loss=29, val_loss=32.40, train_loss=27.10]    Epoch 2:  46%|████▌     | 29/63 [00:00<00:00, 238.35it/s, loss=27.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 238.86it/s, loss=27.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  48%|████▊     | 30/63 [00:00<00:00, 238.56it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 239.55it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  49%|████▉     | 31/63 [00:00<00:00, 239.25it/s, loss=28.5, val_loss=32.40, train_loss=27.10]    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 240.37it/s, loss=28.5, val_loss=32.40, train_loss=27.10]    Epoch 2:  51%|█████     | 32/63 [00:00<00:00, 240.13it/s, loss=28.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 241.37it/s, loss=28.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  52%|█████▏    | 33/63 [00:00<00:00, 241.03it/s, loss=29.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 241.02it/s, loss=29.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  54%|█████▍    | 34/63 [00:00<00:00, 240.72it/s, loss=29.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 241.34it/s, loss=29.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  56%|█████▌    | 35/63 [00:00<00:00, 241.09it/s, loss=29.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 242.07it/s, loss=29.3, val_loss=32.40, train_loss=27.10]    Epoch 2:  57%|█████▋    | 36/63 [00:00<00:00, 241.74it/s, loss=28.9, val_loss=32.40, train_loss=27.10]    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 241.40it/s, loss=28.9, val_loss=32.40, train_loss=27.10]    Epoch 2:  59%|█████▊    | 37/63 [00:00<00:00, 241.15it/s, loss=29.1, val_loss=32.40, train_loss=27.10]    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 241.94it/s, loss=29.1, val_loss=32.40, train_loss=27.10]    Epoch 2:  60%|██████    | 38/63 [00:00<00:00, 241.66it/s, loss=30.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 241.77it/s, loss=30.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  62%|██████▏   | 39/63 [00:00<00:00, 241.52it/s, loss=29.9, val_loss=32.40, train_loss=27.10]    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 242.23it/s, loss=29.9, val_loss=32.40, train_loss=27.10]    Epoch 2:  63%|██████▎   | 40/63 [00:00<00:00, 241.98it/s, loss=30.8, val_loss=32.40, train_loss=27.10]    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 242.58it/s, loss=30.8, val_loss=32.40, train_loss=27.10]    Epoch 2:  65%|██████▌   | 41/63 [00:00<00:00, 242.34it/s, loss=30.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 243.00it/s, loss=30.7, val_loss=32.40, train_loss=27.10]    Epoch 2:  67%|██████▋   | 42/63 [00:00<00:00, 242.74it/s, loss=30, val_loss=32.40, train_loss=27.10]      Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 243.10it/s, loss=30, val_loss=32.40, train_loss=27.10]    Epoch 2:  68%|██████▊   | 43/63 [00:00<00:00, 242.91it/s, loss=28.8, val_loss=32.40, train_loss=27.10]    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 243.63it/s, loss=28.8, val_loss=32.40, train_loss=27.10]    Epoch 2:  70%|██████▉   | 44/63 [00:00<00:00, 243.38it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 243.54it/s, loss=27.6, val_loss=32.40, train_loss=27.10]    Epoch 2:  71%|███████▏  | 45/63 [00:00<00:00, 243.30it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 243.73it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  73%|███████▎  | 46/63 [00:00<00:00, 243.50it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 243.74it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  75%|███████▍  | 47/63 [00:00<00:00, 243.52it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 243.77it/s, loss=27.4, val_loss=32.40, train_loss=27.10]    Epoch 2:  76%|███████▌  | 48/63 [00:00<00:00, 243.56it/s, loss=27.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 243.93it/s, loss=27.2, val_loss=32.40, train_loss=27.10]    Epoch 2:  78%|███████▊  | 49/63 [00:00<00:00, 243.72it/s, loss=26.5, val_loss=32.40, train_loss=27.10]    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 244.04it/s, loss=26.5, val_loss=32.40, train_loss=27.10]    Epoch 2:  79%|███████▉  | 50/63 [00:00<00:00, 243.83it/s, loss=26, val_loss=32.40, train_loss=27.10]      Epoch 2:  81%|████████  | 51/63 [00:00<00:00, 246.32it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  83%|████████▎ | 52/63 [00:00<00:00, 250.31it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  84%|████████▍ | 53/63 [00:00<00:00, 254.34it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  86%|████████▌ | 54/63 [00:00<00:00, 258.37it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  87%|████████▋ | 55/63 [00:00<00:00, 262.39it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  89%|████████▉ | 56/63 [00:00<00:00, 266.40it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  90%|█████████ | 57/63 [00:00<00:00, 270.35it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  92%|█████████▏| 58/63 [00:00<00:00, 274.32it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  94%|█████████▎| 59/63 [00:00<00:00, 278.28it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  95%|█████████▌| 60/63 [00:00<00:00, 282.08it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  97%|█████████▋| 61/63 [00:00<00:00, 286.02it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2:  98%|█████████▊| 62/63 [00:00<00:00, 289.94it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 293.88it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 292.58it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 292.05it/s, loss=26, val_loss=32.40, train_loss=27.10]    Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 285.05it/s, loss=26, val_loss=32.40, train_loss=27.10]
    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         Validate metric           DataLoader 0
    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
             MAE_val             4.60679817199707
             R2_val           -0.0008908510208129883
            val_loss             32.36520004272461
    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────




.. GENERATED FROM PYTHON SOURCE LINES 202-204

7. Plotting the results of the second model
----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 204-209

.. code-block:: default


    plotter = Plotter(single_model_dict, params)
    single_model_figures_dict = plotter.plot_all()
    plotter.show_all(single_model_figures_dict)




.. image-sg:: /auto_examples/images/sphx_glr_plot_two_models_traintest_002.png
   :alt: TabularChannelWiseMultiAttention - Validation R2: -0.001
   :srcset: /auto_examples/images/sphx_glr_plot_two_models_traintest_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Plotting models ['TabularChannelWiseMultiAttention'] ...
    Plotting results of a single model.
    TabularChannelWiseMultiAttention_reals_vs_preds




.. GENERATED FROM PYTHON SOURCE LINES 210-213

8. Comparing the results of the two models
---------------------------------------------
Now we're going to compare the results of the two models. We're using the same steps as when we used Plotter before, but this time we're using the ``all_trained_models`` dictionary which contains both models.

.. GENERATED FROM PYTHON SOURCE LINES 213-218

.. code-block:: default


    comparison_plotter = Plotter(all_trained_models, params)
    comparison_plot_dict = comparison_plotter.plot_all()
    comparison_plotter.show_all(comparison_plot_dict)




.. image-sg:: /auto_examples/images/sphx_glr_plot_two_models_traintest_003.png
   :alt: Model Performance Comparison, R2, MAE
   :srcset: /auto_examples/images/sphx_glr_plot_two_models_traintest_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Plotting models ['ConcatTabularData', 'TabularChannelWiseMultiAttention'] ...
    Plotting comparison plots for 2 models: ['ConcatTabularData', 'TabularChannelWiseMultiAttention']
    compare_tt_models




.. GENERATED FROM PYTHON SOURCE LINES 219-222

9. Saving the metrics of the two models
-----------------------------------------
We can also get the metrics of the two models into a Pandas DataFrame using the :func:`~fusionlibrary.eval_functions.Plotter.get_performance_df` function.

.. GENERATED FROM PYTHON SOURCE LINES 222-224

.. code-block:: default

    performances_df = comparison_plotter.get_performance_df()
    performances_df





.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>R2</th>
          <th>MAE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>ConcatTabularData</th>
          <td>0.037334</td>
          <td>4.126473</td>
        </tr>
        <tr>
          <th>TabularChannelWiseMultiAttention</th>
          <td>-0.000891</td>
          <td>4.606798</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.909 seconds)


.. _sphx_glr_download_auto_examples_plot_two_models_traintest.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_two_models_traintest.py <plot_two_models_traintest.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_two_models_traintest.ipynb <plot_two_models_traintest.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
